{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/chihyuan/Multimodal-Schizo/weights/chinese-bert-wwm-scz were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/chihyuan/Multimodal-Schizo/weights/chinese-bert-wwm-scz and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model_name = \"/home/chihyuan/Multimodal-Schizo/weights/chinese-bert-wwm-scz\"\n",
    "# model_name = \"/home/chihyuan/Multimodal-Schizo/weights/chinese-electra-108g-small-discriminator_CTB8\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "adapter_name = \"sem\"\n",
    "model.add_adapter(adapter_name, config=\"houlsby\")\n",
    "model.train_adapter(adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "BertModel                                                    --\n",
       "├─ModuleDict: 1-1                                            --\n",
       "├─ModuleDict: 1-2                                            --\n",
       "├─BertEmbeddings: 1-3                                        --\n",
       "│    └─Embedding: 2-1                                        (16,226,304)\n",
       "│    └─Embedding: 2-2                                        (393,216)\n",
       "│    └─Embedding: 2-3                                        (1,536)\n",
       "│    └─LayerNorm: 2-4                                        (1,536)\n",
       "│    └─Dropout: 2-5                                          --\n",
       "├─BertEncoder: 1-4                                           --\n",
       "│    └─ModuleList: 2-6                                       --\n",
       "│    │    └─BertLayer: 3-1                                   --\n",
       "│    │    │    └─BertAttention: 4-1                          --\n",
       "│    │    │    │    └─BertSelfAttention: 5-1                 --\n",
       "│    │    │    │    │    └─Linear: 6-1                       590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-1              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-1               12,288\n",
       "│    │    │    │    │    └─Linear: 6-2                       590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-2              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-2               12,288\n",
       "│    │    │    │    │    └─Linear: 6-3                       590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-3              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-3               12,288\n",
       "│    │    │    │    │    └─Dropout: 6-4                      --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-5             --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-4        --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-4         --\n",
       "│    │    │    │    └─BertSelfOutput: 5-2                    --\n",
       "│    │    │    │    │    └─Linear: 6-6                       (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-7                    (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-8                      --\n",
       "│    │    │    │    │    └─ModuleDict: 6-9                   --\n",
       "│    │    │    │    │    └─ModuleDict: 6-10                  --\n",
       "│    │    │    └─BertIntermediate: 4-2                       --\n",
       "│    │    │    │    └─Linear: 5-3                            (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-11                  --\n",
       "│    │    │    │    └─GELUActivation: 5-4                    --\n",
       "│    │    │    └─BertOutput: 4-3                             --\n",
       "│    │    │    │    └─Linear: 5-5                            (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-12                  --\n",
       "│    │    │    │    └─LayerNorm: 5-6                         (1,536)\n",
       "│    │    │    │    └─Dropout: 5-7                           --\n",
       "│    │    │    │    └─ModuleDict: 5-8                        --\n",
       "│    │    │    │    └─ModuleDict: 5-9                        --\n",
       "│    │    └─BertLayer: 3-2                                   --\n",
       "│    │    │    └─BertAttention: 4-4                          --\n",
       "│    │    │    │    └─BertSelfAttention: 5-10                --\n",
       "│    │    │    │    │    └─Linear: 6-13                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-5              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-5               12,288\n",
       "│    │    │    │    │    └─Linear: 6-14                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-6              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-6               12,288\n",
       "│    │    │    │    │    └─Linear: 6-15                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-7              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-7               12,288\n",
       "│    │    │    │    │    └─Dropout: 6-16                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-17            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-8        --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-8         --\n",
       "│    │    │    │    └─BertSelfOutput: 5-11                   --\n",
       "│    │    │    │    │    └─Linear: 6-18                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-19                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-20                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-21                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-22                  --\n",
       "│    │    │    └─BertIntermediate: 4-5                       --\n",
       "│    │    │    │    └─Linear: 5-12                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-23                  --\n",
       "│    │    │    │    └─GELUActivation: 5-13                   --\n",
       "│    │    │    └─BertOutput: 4-6                             --\n",
       "│    │    │    │    └─Linear: 5-14                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-24                  --\n",
       "│    │    │    │    └─LayerNorm: 5-15                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-16                          --\n",
       "│    │    │    │    └─ModuleDict: 5-17                       --\n",
       "│    │    │    │    └─ModuleDict: 5-18                       --\n",
       "│    │    └─BertLayer: 3-3                                   --\n",
       "│    │    │    └─BertAttention: 4-7                          --\n",
       "│    │    │    │    └─BertSelfAttention: 5-19                --\n",
       "│    │    │    │    │    └─Linear: 6-25                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-9              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-9               12,288\n",
       "│    │    │    │    │    └─Linear: 6-26                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-10             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-10              12,288\n",
       "│    │    │    │    │    └─Linear: 6-27                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-11             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-11              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-28                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-29            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-12       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-12        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-20                   --\n",
       "│    │    │    │    │    └─Linear: 6-30                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-31                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-32                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-33                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-34                  --\n",
       "│    │    │    └─BertIntermediate: 4-8                       --\n",
       "│    │    │    │    └─Linear: 5-21                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-35                  --\n",
       "│    │    │    │    └─GELUActivation: 5-22                   --\n",
       "│    │    │    └─BertOutput: 4-9                             --\n",
       "│    │    │    │    └─Linear: 5-23                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-36                  --\n",
       "│    │    │    │    └─LayerNorm: 5-24                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-25                          --\n",
       "│    │    │    │    └─ModuleDict: 5-26                       --\n",
       "│    │    │    │    └─ModuleDict: 5-27                       --\n",
       "│    │    └─BertLayer: 3-4                                   --\n",
       "│    │    │    └─BertAttention: 4-10                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-28                --\n",
       "│    │    │    │    │    └─Linear: 6-37                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-13             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-13              12,288\n",
       "│    │    │    │    │    └─Linear: 6-38                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-14             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-14              12,288\n",
       "│    │    │    │    │    └─Linear: 6-39                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-15             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-15              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-40                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-41            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-16       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-16        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-29                   --\n",
       "│    │    │    │    │    └─Linear: 6-42                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-43                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-44                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-45                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-46                  --\n",
       "│    │    │    └─BertIntermediate: 4-11                      --\n",
       "│    │    │    │    └─Linear: 5-30                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-47                  --\n",
       "│    │    │    │    └─GELUActivation: 5-31                   --\n",
       "│    │    │    └─BertOutput: 4-12                            --\n",
       "│    │    │    │    └─Linear: 5-32                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-48                  --\n",
       "│    │    │    │    └─LayerNorm: 5-33                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-34                          --\n",
       "│    │    │    │    └─ModuleDict: 5-35                       --\n",
       "│    │    │    │    └─ModuleDict: 5-36                       --\n",
       "│    │    └─BertLayer: 3-5                                   --\n",
       "│    │    │    └─BertAttention: 4-13                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-37                --\n",
       "│    │    │    │    │    └─Linear: 6-49                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-17             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-17              12,288\n",
       "│    │    │    │    │    └─Linear: 6-50                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-18             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-18              12,288\n",
       "│    │    │    │    │    └─Linear: 6-51                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-19             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-19              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-52                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-53            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-20       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-20        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-38                   --\n",
       "│    │    │    │    │    └─Linear: 6-54                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-55                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-56                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-57                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-58                  --\n",
       "│    │    │    └─BertIntermediate: 4-14                      --\n",
       "│    │    │    │    └─Linear: 5-39                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-59                  --\n",
       "│    │    │    │    └─GELUActivation: 5-40                   --\n",
       "│    │    │    └─BertOutput: 4-15                            --\n",
       "│    │    │    │    └─Linear: 5-41                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-60                  --\n",
       "│    │    │    │    └─LayerNorm: 5-42                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-43                          --\n",
       "│    │    │    │    └─ModuleDict: 5-44                       --\n",
       "│    │    │    │    └─ModuleDict: 5-45                       --\n",
       "│    │    └─BertLayer: 3-6                                   --\n",
       "│    │    │    └─BertAttention: 4-16                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-46                --\n",
       "│    │    │    │    │    └─Linear: 6-61                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-21             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-21              12,288\n",
       "│    │    │    │    │    └─Linear: 6-62                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-22             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-22              12,288\n",
       "│    │    │    │    │    └─Linear: 6-63                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-23             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-23              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-64                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-65            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-24       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-24        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-47                   --\n",
       "│    │    │    │    │    └─Linear: 6-66                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-67                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-68                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-69                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-70                  --\n",
       "│    │    │    └─BertIntermediate: 4-17                      --\n",
       "│    │    │    │    └─Linear: 5-48                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-71                  --\n",
       "│    │    │    │    └─GELUActivation: 5-49                   --\n",
       "│    │    │    └─BertOutput: 4-18                            --\n",
       "│    │    │    │    └─Linear: 5-50                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-72                  --\n",
       "│    │    │    │    └─LayerNorm: 5-51                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-52                          --\n",
       "│    │    │    │    └─ModuleDict: 5-53                       --\n",
       "│    │    │    │    └─ModuleDict: 5-54                       --\n",
       "│    │    └─BertLayer: 3-7                                   --\n",
       "│    │    │    └─BertAttention: 4-19                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-55                --\n",
       "│    │    │    │    │    └─Linear: 6-73                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-25             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-25              12,288\n",
       "│    │    │    │    │    └─Linear: 6-74                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-26             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-26              12,288\n",
       "│    │    │    │    │    └─Linear: 6-75                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-27             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-27              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-76                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-77            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-28       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-28        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-56                   --\n",
       "│    │    │    │    │    └─Linear: 6-78                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-79                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-80                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-81                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-82                  --\n",
       "│    │    │    └─BertIntermediate: 4-20                      --\n",
       "│    │    │    │    └─Linear: 5-57                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-83                  --\n",
       "│    │    │    │    └─GELUActivation: 5-58                   --\n",
       "│    │    │    └─BertOutput: 4-21                            --\n",
       "│    │    │    │    └─Linear: 5-59                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-84                  --\n",
       "│    │    │    │    └─LayerNorm: 5-60                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-61                          --\n",
       "│    │    │    │    └─ModuleDict: 5-62                       --\n",
       "│    │    │    │    └─ModuleDict: 5-63                       --\n",
       "│    │    └─BertLayer: 3-8                                   --\n",
       "│    │    │    └─BertAttention: 4-22                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-64                --\n",
       "│    │    │    │    │    └─Linear: 6-85                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-29             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-29              12,288\n",
       "│    │    │    │    │    └─Linear: 6-86                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-30             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-30              12,288\n",
       "│    │    │    │    │    └─Linear: 6-87                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-31             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-31              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-88                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-89            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-32       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-32        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-65                   --\n",
       "│    │    │    │    │    └─Linear: 6-90                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-91                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-92                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-93                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-94                  --\n",
       "│    │    │    └─BertIntermediate: 4-23                      --\n",
       "│    │    │    │    └─Linear: 5-66                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-95                  --\n",
       "│    │    │    │    └─GELUActivation: 5-67                   --\n",
       "│    │    │    └─BertOutput: 4-24                            --\n",
       "│    │    │    │    └─Linear: 5-68                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-96                  --\n",
       "│    │    │    │    └─LayerNorm: 5-69                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-70                          --\n",
       "│    │    │    │    └─ModuleDict: 5-71                       --\n",
       "│    │    │    │    └─ModuleDict: 5-72                       --\n",
       "│    │    └─BertLayer: 3-9                                   --\n",
       "│    │    │    └─BertAttention: 4-25                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-73                --\n",
       "│    │    │    │    │    └─Linear: 6-97                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-33             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-33              12,288\n",
       "│    │    │    │    │    └─Linear: 6-98                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-34             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-34              12,288\n",
       "│    │    │    │    │    └─Linear: 6-99                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-35             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-35              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-100                    --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-101           --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-36       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-36        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-74                   --\n",
       "│    │    │    │    │    └─Linear: 6-102                     (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-103                  (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-104                    --\n",
       "│    │    │    │    │    └─ModuleDict: 6-105                 --\n",
       "│    │    │    │    │    └─ModuleDict: 6-106                 --\n",
       "│    │    │    └─BertIntermediate: 4-26                      --\n",
       "│    │    │    │    └─Linear: 5-75                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-107                 --\n",
       "│    │    │    │    └─GELUActivation: 5-76                   --\n",
       "│    │    │    └─BertOutput: 4-27                            --\n",
       "│    │    │    │    └─Linear: 5-77                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-108                 --\n",
       "│    │    │    │    └─LayerNorm: 5-78                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-79                          --\n",
       "│    │    │    │    └─ModuleDict: 5-80                       --\n",
       "│    │    │    │    └─ModuleDict: 5-81                       --\n",
       "│    │    └─BertLayer: 3-10                                  --\n",
       "│    │    │    └─BertAttention: 4-28                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-82                --\n",
       "│    │    │    │    │    └─Linear: 6-109                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-37             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-37              12,288\n",
       "│    │    │    │    │    └─Linear: 6-110                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-38             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-38              12,288\n",
       "│    │    │    │    │    └─Linear: 6-111                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-39             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-39              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-112                    --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-113           --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-40       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-40        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-83                   --\n",
       "│    │    │    │    │    └─Linear: 6-114                     (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-115                  (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-116                    --\n",
       "│    │    │    │    │    └─ModuleDict: 6-117                 --\n",
       "│    │    │    │    │    └─ModuleDict: 6-118                 --\n",
       "│    │    │    └─BertIntermediate: 4-29                      --\n",
       "│    │    │    │    └─Linear: 5-84                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-119                 --\n",
       "│    │    │    │    └─GELUActivation: 5-85                   --\n",
       "│    │    │    └─BertOutput: 4-30                            --\n",
       "│    │    │    │    └─Linear: 5-86                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-120                 --\n",
       "│    │    │    │    └─LayerNorm: 5-87                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-88                          --\n",
       "│    │    │    │    └─ModuleDict: 5-89                       --\n",
       "│    │    │    │    └─ModuleDict: 5-90                       --\n",
       "│    │    └─BertLayer: 3-11                                  --\n",
       "│    │    │    └─BertAttention: 4-31                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-91                --\n",
       "│    │    │    │    │    └─Linear: 6-121                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-41             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-41              12,288\n",
       "│    │    │    │    │    └─Linear: 6-122                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-42             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-42              12,288\n",
       "│    │    │    │    │    └─Linear: 6-123                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-43             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-43              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-124                    --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-125           --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-44       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-44        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-92                   --\n",
       "│    │    │    │    │    └─Linear: 6-126                     (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-127                  (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-128                    --\n",
       "│    │    │    │    │    └─ModuleDict: 6-129                 --\n",
       "│    │    │    │    │    └─ModuleDict: 6-130                 --\n",
       "│    │    │    └─BertIntermediate: 4-32                      --\n",
       "│    │    │    │    └─Linear: 5-93                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-131                 --\n",
       "│    │    │    │    └─GELUActivation: 5-94                   --\n",
       "│    │    │    └─BertOutput: 4-33                            --\n",
       "│    │    │    │    └─Linear: 5-95                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-132                 --\n",
       "│    │    │    │    └─LayerNorm: 5-96                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-97                          --\n",
       "│    │    │    │    └─ModuleDict: 5-98                       --\n",
       "│    │    │    │    └─ModuleDict: 5-99                       --\n",
       "│    │    └─BertLayer: 3-12                                  --\n",
       "│    │    │    └─BertAttention: 4-34                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-100               --\n",
       "│    │    │    │    │    └─Linear: 6-133                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-45             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-45              12,288\n",
       "│    │    │    │    │    └─Linear: 6-134                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-46             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-46              12,288\n",
       "│    │    │    │    │    └─Linear: 6-135                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-47             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-47              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-136                    --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-137           --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-48       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-48        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-101                  --\n",
       "│    │    │    │    │    └─Linear: 6-138                     (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-139                  (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-140                    --\n",
       "│    │    │    │    │    └─ModuleDict: 6-141                 --\n",
       "│    │    │    │    │    └─ModuleDict: 6-142                 --\n",
       "│    │    │    └─BertIntermediate: 4-35                      --\n",
       "│    │    │    │    └─Linear: 5-102                          (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-143                 --\n",
       "│    │    │    │    └─GELUActivation: 5-103                  --\n",
       "│    │    │    └─BertOutput: 4-36                            --\n",
       "│    │    │    │    └─Linear: 5-104                          (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-144                 --\n",
       "│    │    │    │    └─LayerNorm: 5-105                       (1,536)\n",
       "│    │    │    │    └─Dropout: 5-106                         --\n",
       "│    │    │    │    └─ModuleDict: 5-107                      --\n",
       "│    │    │    │    └─ModuleDict: 5-108                      --\n",
       "├─BertPooler: 1-5                                            --\n",
       "│    └─Linear: 2-7                                           (590,592)\n",
       "│    └─Tanh: 2-8                                             --\n",
       "├─PrefixTuningPool: 1-6                                      --\n",
       "│    └─ModuleDict: 2-9                                       --\n",
       "=====================================================================================\n",
       "Total params: 102,710,016\n",
       "Trainable params: 442,368\n",
       "Non-trainable params: 102,267,648\n",
       "====================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(model.dummy_inputs[\"input_ids\"].shape)\n",
    "model(**model.dummy_inputs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "bottleneck = 48\n",
    "print(bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2695, -0.2492, -0.0831,  ..., -0.0351,  0.7340,  0.1247],\n",
       "         [ 0.7348,  0.1523,  0.2606,  ..., -0.1404,  0.9398,  0.1040],\n",
       "         [ 0.9366, -0.1383,  0.6437,  ..., -0.0716,  0.6156,  0.0723],\n",
       "         [ 0.8001, -0.1691,  0.2462,  ...,  0.4305,  0.3796,  0.1382],\n",
       "         [ 0.6385, -0.2078,  0.6147,  ..., -0.0214,  0.7147,  0.2676]],\n",
       "\n",
       "        [[ 0.5024, -0.2017,  0.0671,  ..., -0.0914,  0.2362, -0.3528],\n",
       "         [ 0.4034, -0.4648,  0.6177,  ...,  0.2243,  0.1670, -0.0913],\n",
       "         [ 0.6191, -0.2262,  0.3571,  ...,  0.6194,  0.1060,  0.1200],\n",
       "         [ 0.1643, -0.3347,  0.1732,  ...,  0.1812,  0.1764, -0.1784],\n",
       "         [ 0.4976, -0.5537,  0.2065,  ...,  0.0263,  0.2702, -0.1939]],\n",
       "\n",
       "        [[ 0.2947, -0.5224,  0.7865,  ...,  0.1304, -0.0791, -0.1319],\n",
       "         [ 0.6163, -0.3831,  0.7307,  ...,  0.0494, -0.8643, -0.3173],\n",
       "         [-0.2894, -0.4522,  0.9227,  ...,  0.4364, -0.5582,  0.0109],\n",
       "         [ 0.2058, -0.2946,  0.7202,  ...,  0.1398, -0.5434, -0.1115],\n",
       "         [ 0.4102, -0.5085,  0.7268,  ...,  0.3014, -0.3204,  0.0106]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.1746,  0.1277,  0.1054,  ..., -0.2330,  0.4706,  0.2159],\n",
       "        [ 0.2315,  0.2028, -0.1015,  ..., -0.1551, -0.0219,  0.2033],\n",
       "        [ 0.2957, -0.0801,  0.3230,  ...,  0.3106,  0.2263,  0.0960]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=(tensor([[[-0.0709,  0.4743, -0.6127,  ...,  0.9273, -0.0567, -0.0525],\n",
       "         [-0.4144,  0.3176, -0.0180,  ...,  0.7743, -0.4012,  0.6330],\n",
       "         [ 0.6165,  0.0000, -0.4190,  ...,  1.3247,  0.1765,  0.6666],\n",
       "         [ 0.7094,  0.1524,  0.1136,  ...,  1.6618, -0.1672,  1.0848],\n",
       "         [-0.3168,  0.5922, -0.0758,  ...,  0.7579,  0.6435,  0.5169]],\n",
       "\n",
       "        [[-0.2909,  0.4406, -0.0434,  ...,  1.1333, -0.2927,  0.1309],\n",
       "         [ 0.0972, -0.0223,  0.0130,  ...,  0.9420,  0.0252,  0.0000],\n",
       "         [ 0.2945,  0.3630, -0.3140,  ...,  1.4110,  0.0000,  0.4522],\n",
       "         [ 0.7094,  0.1524,  0.1136,  ...,  0.0000, -0.0000,  1.0848],\n",
       "         [ 0.5170,  0.5643, -0.0000,  ...,  0.9029,  0.5088,  0.2972]],\n",
       "\n",
       "        [[ 0.3221,  0.0000, -0.2156,  ...,  0.0000, -0.3957, -0.0330],\n",
       "         [ 0.4009,  0.3921, -0.2133,  ...,  1.0170, -0.4696,  0.3946],\n",
       "         [ 0.0000,  0.4129, -0.0000,  ...,  1.3247,  0.1765,  0.0000],\n",
       "         [ 0.3282,  0.0781, -0.4556,  ...,  1.4160, -0.2918,  1.2290],\n",
       "         [-0.1957,  0.3713, -0.3055,  ...,  0.9995,  0.9066,  0.2769]]]), tensor([[[ 0.2593,  0.6469, -0.3519,  ...,  0.3491,  0.0593, -0.1516],\n",
       "         [-0.0273,  0.4874, -0.3586,  ...,  0.2679, -0.5020,  0.3267],\n",
       "         [ 0.7056,  1.1544, -0.9265,  ...,  0.9600,  0.0653,  0.4632],\n",
       "         [ 0.6016,  1.0412, -0.1138,  ...,  1.0287, -0.3000,  0.6338],\n",
       "         [-0.0715,  1.4206, -0.4653,  ...,  0.4011,  0.5282,  0.1997]],\n",
       "\n",
       "        [[ 0.0908,  0.4741, -0.5794,  ...,  0.4610,  0.1343, -0.2052],\n",
       "         [-0.1753,  0.8244, -0.4177,  ...,  0.2868,  0.1979,  0.2099],\n",
       "         [ 0.5389,  0.9114, -0.7619,  ...,  1.2385,  0.2523,  0.7230],\n",
       "         [ 0.4083,  0.5355, -0.3408,  ...,  0.1139,  0.0343,  1.3229],\n",
       "         [ 0.3270,  0.3486, -0.5506,  ...,  0.2423,  0.6121,  0.1081]],\n",
       "\n",
       "        [[ 0.2852,  0.3006, -0.4977,  ..., -0.0178, -0.4670, -0.2471],\n",
       "         [ 0.1341,  1.1050, -0.4945,  ...,  0.3448, -0.3921,  0.1026],\n",
       "         [-0.1649,  1.3319, -0.5680,  ...,  0.6637,  0.1768,  0.0355],\n",
       "         [ 0.0215,  0.3026, -0.3755,  ...,  0.4193, -0.3653,  1.2722],\n",
       "         [-0.5674,  1.0958, -0.6138,  ...,  0.1182,  1.2884,  0.2410]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3060,  0.4692, -0.4600,  ..., -0.1385,  0.1390,  0.0750],\n",
       "         [ 0.1948,  0.3660, -0.3778,  ..., -0.1879,  0.0926,  0.3982],\n",
       "         [ 0.6446,  1.0056, -0.8990,  ...,  0.4327,  0.4650,  0.3160],\n",
       "         [ 0.6859,  0.7585, -0.3322,  ...,  0.4488,  0.0638,  0.8723],\n",
       "         [-0.0904,  1.1004, -0.2691,  ...,  0.0099,  0.7012,  0.4258]],\n",
       "\n",
       "        [[-0.2061,  0.2308, -0.4232,  ..., -0.2312,  0.3510,  0.0974],\n",
       "         [-0.0386,  0.4749, -0.0760,  ..., -0.1819,  0.5173,  0.4380],\n",
       "         [ 0.6964,  0.9922, -0.4918,  ...,  0.9037,  0.7928,  0.9278],\n",
       "         [ 0.3040,  0.2747, -0.3491,  ...,  0.0953,  0.4789,  1.3414],\n",
       "         [ 0.2717,  0.1545, -0.7033,  ..., -0.1503,  0.9974,  0.4511]],\n",
       "\n",
       "        [[ 0.1706,  0.0836, -0.5174,  ..., -0.2542,  0.0509,  0.0780],\n",
       "         [ 0.4162,  0.4037, -0.3937,  ...,  0.4962,  0.1247,  0.0694],\n",
       "         [ 0.0391,  1.0213, -0.3963,  ...,  0.6960,  0.1690,  0.1973],\n",
       "         [ 0.3528,  0.2501, -0.1570,  ..., -0.0108, -0.1028,  1.5770],\n",
       "         [-0.3349,  0.9046, -0.7339,  ...,  0.2827,  1.5746,  0.3407]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3064,  0.0505, -0.4647,  ..., -0.1794, -0.3494,  0.3315],\n",
       "         [ 0.4488,  0.2815, -0.3030,  ..., -0.4656,  0.0097,  0.1044],\n",
       "         [ 0.6343,  0.8130, -0.9595,  ..., -0.1115, -0.0484,  0.3694],\n",
       "         [ 0.5605,  0.9426, -0.4559,  ..., -0.4717, -0.5006,  0.5984],\n",
       "         [ 0.1702,  0.9419, -0.5591,  ..., -0.4230, -0.0323,  0.1741]],\n",
       "\n",
       "        [[-0.3448,  0.1439, -0.5048,  ..., -0.2277,  0.0057, -0.0307],\n",
       "         [ 0.4442,  0.5935,  0.0336,  ..., -0.5551,  0.2419,  0.0020],\n",
       "         [ 1.0081,  0.9045, -0.4588,  ...,  0.6466,  0.3348,  0.3109],\n",
       "         [ 0.3683,  0.3749, -0.3803,  ..., -0.4658,  0.1851,  0.5790],\n",
       "         [ 0.4736,  0.3831, -0.7074,  ..., -0.5486,  0.7063,  0.1938]],\n",
       "\n",
       "        [[ 0.2398,  0.2396, -0.6512,  ..., -0.1370, -0.0674, -0.2165],\n",
       "         [ 0.6815,  0.3649, -0.3284,  ...,  0.1915,  0.0866, -0.0708],\n",
       "         [ 0.3300,  0.6968, -0.0802,  ...,  0.4928, -0.2089,  0.0499],\n",
       "         [ 0.2929,  0.3883, -0.2235,  ..., -0.4500, -0.5866,  0.7381],\n",
       "         [-0.0307,  0.7987, -0.7917,  ...,  0.1543,  0.9192, -0.1014]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5384, -0.1142, -0.5027,  ...,  0.1498, -0.6163,  0.1500],\n",
       "         [ 0.4072,  0.4640, -0.3829,  ..., -0.3568, -0.1209, -0.2975],\n",
       "         [ 0.9597,  1.0675, -0.9529,  ..., -0.1118, -0.1269, -0.1497],\n",
       "         [ 0.7533,  0.8829, -0.6978,  ..., -0.4265, -0.7151, -0.0028],\n",
       "         [ 0.3441,  0.8518, -0.9201,  ..., -0.0351, -0.2205, -0.1015]],\n",
       "\n",
       "        [[-0.1952, -0.2821, -0.5955,  ..., -0.3872, -0.2776, -0.5327],\n",
       "         [ 0.2200,  0.3465, -0.5353,  ...,  0.3656,  0.3986, -0.0889],\n",
       "         [ 1.1536,  0.5352, -0.8585,  ...,  0.8325, -0.1415, -0.0469],\n",
       "         [ 0.3706,  0.8612, -0.6658,  ...,  0.3677, -0.2633, -0.0609],\n",
       "         [ 0.6438,  0.4490, -1.1091,  ...,  0.2365,  0.1355, -0.1850]],\n",
       "\n",
       "        [[ 0.4638,  0.3504, -0.7964,  ..., -0.0721, -0.3919, -0.6663],\n",
       "         [ 0.8013,  0.1560, -0.3920,  ...,  0.3144, -0.1238, -0.0155],\n",
       "         [ 0.4733,  0.6416, -0.0142,  ...,  0.6133, -0.5405, -0.7387],\n",
       "         [ 0.6778,  0.4665, -0.3979,  ...,  0.5650, -0.9294, -0.0444],\n",
       "         [ 0.2055,  0.5282, -1.0287,  ...,  0.5733,  0.2383, -0.6780]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.1163, -0.1385,  0.0107,  ...,  0.2186, -0.1284, -0.0069],\n",
       "         [ 0.9492,  0.4753,  0.3911,  ...,  0.2511,  0.5676, -0.0634],\n",
       "         [ 1.5632,  0.8406, -0.3104,  ...,  0.2783,  0.6570, -0.2662],\n",
       "         [ 0.6338,  0.6926, -0.0270,  ..., -0.0430,  0.2453,  0.0888],\n",
       "         [ 0.8654,  0.4079, -0.3003,  ..., -0.0230,  0.4193,  0.0903]],\n",
       "\n",
       "        [[ 0.6232,  0.3889, -0.5294,  ..., -0.6142,  0.7154, -0.8053],\n",
       "         [ 0.8238,  0.7582, -0.4288,  ...,  0.0905,  1.3886,  0.1645],\n",
       "         [ 2.1281,  0.7673, -0.8028,  ...,  0.0092,  1.4204,  0.1945],\n",
       "         [ 0.6253,  0.9716, -0.6165,  ..., -0.5313,  1.2058,  0.1667],\n",
       "         [ 1.3879,  0.8657, -0.8170,  ...,  0.3394,  1.1136, -0.2491]],\n",
       "\n",
       "        [[ 1.0680,  0.0039, -0.3691,  ...,  0.3576, -0.1748, -0.2462],\n",
       "         [ 1.2612, -0.1654, -0.0956,  ...,  0.8944,  0.6424,  0.1004],\n",
       "         [ 1.1236,  0.5204, -0.1506,  ...,  1.0101,  0.0757, -0.0545],\n",
       "         [ 0.9995,  0.0749, -0.3043,  ...,  0.7128, -0.4217,  0.2343],\n",
       "         [ 0.7548,  0.4233, -0.3777,  ...,  0.5371,  0.1445,  0.1535]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.8222, -0.0725, -0.1843,  ..., -0.2234, -0.8435, -0.2133],\n",
       "         [ 0.8136,  0.3702,  0.3484,  ...,  0.7859,  0.2428, -0.0962],\n",
       "         [ 1.0182,  0.4912, -0.1342,  ...,  0.1054,  0.3251, -0.3045],\n",
       "         [ 0.3958,  0.7215,  0.2397,  ...,  0.5565,  0.2745, -0.1294],\n",
       "         [ 0.5279,  0.0802, -0.2190,  ...,  0.2178,  0.1257,  0.3829]],\n",
       "\n",
       "        [[ 1.0332,  0.6171,  0.0258,  ..., -0.2236,  0.2606, -1.2196],\n",
       "         [ 1.2260,  0.8874, -0.1171,  ...,  0.7888,  0.5438, -0.2615],\n",
       "         [ 1.7425,  0.8916, -0.4842,  ...,  0.7560,  0.8755, -0.1047],\n",
       "         [ 1.2780,  1.1538, -0.1391,  ..., -0.4159,  0.3397, -0.4077],\n",
       "         [ 1.9279,  0.1489, -0.5270,  ..., -0.0418,  0.4819, -0.5430]],\n",
       "\n",
       "        [[ 1.3267,  0.3500, -0.5114,  ..., -0.1321, -0.6811, -0.9850],\n",
       "         [ 1.1815,  0.0466, -0.2532,  ...,  0.5213,  0.1521, -0.2337],\n",
       "         [ 1.0232,  0.3180,  0.0217,  ...,  0.3929,  0.4509,  0.0695],\n",
       "         [ 1.5056,  0.3472, -0.3021,  ..., -0.0889, -0.1917, -0.2149],\n",
       "         [ 1.4193,  0.5990, -0.2987,  ..., -0.3926,  0.1524, -0.5311]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4459, -0.1547, -0.5590,  ..., -0.6651, -0.7562,  0.5126],\n",
       "         [ 0.4487, -0.0299, -0.2023,  ...,  0.6951, -0.6285,  0.5208],\n",
       "         [ 0.5991,  0.3837, -0.2974,  ..., -0.3278, -0.3833, -0.2281],\n",
       "         [ 0.0830, -0.0582, -0.1733,  ...,  0.6150, -0.1403,  0.2848],\n",
       "         [ 0.2810,  0.0020, -0.3884,  ...,  0.2129,  0.0607,  0.6145]],\n",
       "\n",
       "        [[ 1.1266,  0.1895, -0.1268,  ..., -1.1985, -0.1572, -0.6379],\n",
       "         [ 0.3639, -0.1043,  0.3009,  ...,  0.6104, -0.6265,  0.4328],\n",
       "         [ 0.7800,  1.2520, -0.2748,  ...,  0.2142, -0.6084,  0.5287],\n",
       "         [ 1.1490,  0.1943, -0.3119,  ..., -0.6719, -0.9087,  0.1882],\n",
       "         [ 1.4752, -0.3994, -0.3890,  ..., -0.6493, -0.7040,  0.0043]],\n",
       "\n",
       "        [[ 0.9253, -0.1046, -1.2725,  ..., -0.4090, -1.0732,  0.1871],\n",
       "         [ 1.0820,  0.5270, -0.6710,  ...,  0.3041, -1.3032,  0.2922],\n",
       "         [ 0.7423,  0.0904, -0.4813,  ...,  0.0625, -0.8685,  0.3393],\n",
       "         [ 0.8432,  0.1029, -0.7417,  ...,  0.1860, -1.3138,  0.1821],\n",
       "         [ 1.0774,  0.4849, -1.0878,  ...,  0.4317, -1.4456,  0.0765]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.8228, -0.6885, -0.9591,  ..., -0.2890, -0.5460,  0.9189],\n",
       "         [ 1.1509,  0.0963, -0.5580,  ..., -0.0032, -0.9525,  0.7469],\n",
       "         [ 1.0723,  0.0585, -0.4584,  ..., -0.2724, -0.7189,  0.1101],\n",
       "         [ 0.5995, -0.0492, -0.7717,  ...,  0.4294, -0.1633,  0.3777],\n",
       "         [ 0.7877, -0.1561, -1.0974,  ...,  0.4866,  0.1776,  0.7930]],\n",
       "\n",
       "        [[ 0.9273,  0.3231, -0.3593,  ..., -1.7030, -0.4646, -0.5689],\n",
       "         [-0.0534, -0.3788,  0.2794,  ..., -0.3477, -0.5443,  0.5541],\n",
       "         [ 0.6687,  0.2624, -0.1110,  ..., -0.7088, -0.9804,  0.4183],\n",
       "         [ 0.5253, -0.0369, -0.5187,  ..., -1.2060, -0.9919,  0.2292],\n",
       "         [ 1.1264, -0.6248, -0.6188,  ..., -1.4902, -0.7268,  0.4300]],\n",
       "\n",
       "        [[ 0.0797, -0.8606, -1.5204,  ...,  0.2760, -0.8748,  0.2611],\n",
       "         [ 0.7252, -0.1691, -0.5739,  ..., -0.5229, -1.3441,  0.4765],\n",
       "         [-0.0684,  0.4346, -0.4060,  ..., -0.5136, -1.1859,  0.4574],\n",
       "         [ 0.3262, -0.5354, -0.8173,  ..., -0.4141, -1.6782,  0.3324],\n",
       "         [ 0.8368, -0.1618, -1.0651,  ...,  0.1848, -1.4539,  0.3439]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5891,  0.3667, -0.5974,  ..., -0.0429, -0.5857,  1.0307],\n",
       "         [ 1.1284,  1.3929, -1.1336,  ..., -0.5304, -0.9164,  1.2072],\n",
       "         [ 1.5328,  0.6903, -1.1767,  ..., -0.4475, -1.0014,  0.5524],\n",
       "         [ 0.5295,  0.8217, -1.1584,  ..., -0.0220, -0.4325,  0.9818],\n",
       "         [ 0.5013,  1.1395, -0.9472,  ...,  0.0251,  0.2788,  1.2784]],\n",
       "\n",
       "        [[ 0.1970,  1.1094, -0.6167,  ..., -0.5300, -0.7241, -0.5663],\n",
       "         [-0.1905,  0.4823, -0.2112,  ...,  0.2052, -1.0470,  0.0519],\n",
       "         [ 0.6699,  1.3100, -0.3728,  ..., -0.3901, -1.2964, -0.0518],\n",
       "         [ 0.0891,  1.4398, -0.8201,  ..., -0.6911, -1.1133, -0.0884],\n",
       "         [ 0.4761,  0.5834, -0.4585,  ..., -0.7144, -1.0769,  0.0683]],\n",
       "\n",
       "        [[ 0.3381,  0.4287, -1.0395,  ...,  0.2382, -1.3142,  0.1219],\n",
       "         [ 0.2537,  0.7330, -0.0171,  ..., -0.7071, -1.4006, -0.3478],\n",
       "         [ 0.1113,  1.5070, -0.7316,  ..., -0.9202, -1.1310, -0.3152],\n",
       "         [ 0.4729,  0.7824, -0.9735,  ..., -0.5464, -1.6174, -0.6065],\n",
       "         [ 1.0440,  1.1707, -0.6980,  ..., -0.1723, -1.6784,  0.0773]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 6.4444e-01,  3.2985e-01, -2.6218e-01,  ..., -1.1576e-03,\n",
       "           7.8905e-02,  2.9216e-01],\n",
       "         [ 1.3601e+00,  8.7552e-01, -9.9020e-01,  ..., -3.9047e-01,\n",
       "          -6.5199e-01,  4.2905e-01],\n",
       "         [ 1.4843e+00,  4.1576e-01, -6.4883e-01,  ..., -4.3394e-01,\n",
       "          -9.7256e-01,  2.6270e-01],\n",
       "         [ 9.8589e-01,  3.7169e-01, -8.6865e-01,  ...,  1.8244e-02,\n",
       "          -3.5052e-01,  5.4652e-01],\n",
       "         [ 7.6582e-01,  5.1322e-01, -6.8364e-01,  ...,  1.8082e-01,\n",
       "          -1.5849e-02,  9.1526e-01]],\n",
       "\n",
       "        [[ 1.1983e+00,  9.5916e-01, -4.3351e-01,  ..., -9.4146e-02,\n",
       "          -2.9397e-01, -1.0980e+00],\n",
       "         [ 8.3734e-01,  2.9844e-01, -3.3898e-01,  ...,  7.4258e-01,\n",
       "          -8.1811e-01, -2.4818e-01],\n",
       "         [ 1.1377e+00,  1.0191e+00, -5.2600e-01,  ...,  6.5757e-01,\n",
       "          -3.6487e-01, -3.1012e-01],\n",
       "         [ 1.8255e-01,  1.1143e+00, -1.0007e+00,  ...,  1.5208e-01,\n",
       "          -9.7954e-01, -3.6955e-01],\n",
       "         [ 1.3566e+00,  4.5802e-01, -6.6775e-01,  ...,  6.5158e-02,\n",
       "           3.1058e-02, -5.2079e-01]],\n",
       "\n",
       "        [[ 1.0430e+00, -3.6547e-01, -1.0802e+00,  ...,  8.4677e-01,\n",
       "          -1.5801e+00, -1.2495e-01],\n",
       "         [ 1.3998e+00, -1.1303e-01, -5.8273e-01,  ..., -1.7589e-01,\n",
       "          -1.8332e+00, -5.3189e-01],\n",
       "         [ 3.2490e-01,  3.3386e-01, -5.2220e-01,  ...,  1.5497e-01,\n",
       "          -1.5213e+00, -4.6065e-01],\n",
       "         [ 8.7635e-01, -2.1040e-01, -7.0003e-01,  ...,  2.1556e-01,\n",
       "          -2.0730e+00, -2.5621e-01],\n",
       "         [ 1.7012e+00,  1.0240e-01, -7.1330e-01,  ...,  4.5184e-01,\n",
       "          -1.9660e+00,  1.3178e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 7.6483e-01, -1.4806e-01, -1.7969e-01,  ..., -6.3114e-01,\n",
       "           1.7448e-02,  8.4487e-03],\n",
       "         [ 1.1259e+00,  2.7315e-01, -1.9284e-01,  ..., -5.6315e-01,\n",
       "          -1.0982e-01, -1.6970e-01],\n",
       "         [ 1.5981e+00, -1.8159e-01,  3.7733e-01,  ..., -4.1578e-01,\n",
       "          -5.8221e-01, -1.5540e-01],\n",
       "         [ 9.4999e-01, -1.9327e-01,  1.4893e-01,  ..., -2.3609e-01,\n",
       "          -1.1964e-01,  1.2561e-01],\n",
       "         [ 9.6590e-01, -1.3694e-01,  4.2304e-01,  ..., -1.9360e-03,\n",
       "          -1.9625e-01,  1.6949e-01]],\n",
       "\n",
       "        [[ 1.2487e+00,  2.2916e-01,  2.4928e-02,  ..., -3.9246e-01,\n",
       "          -2.2937e-01, -1.4115e+00],\n",
       "         [ 8.7426e-01, -2.4318e-01,  1.8663e-01,  ...,  2.9414e-01,\n",
       "          -4.3812e-01, -4.6645e-01],\n",
       "         [ 1.0393e+00,  6.3122e-02,  1.3110e-01,  ...,  1.5348e-01,\n",
       "           9.7413e-02, -2.7406e-01],\n",
       "         [ 4.2003e-01, -9.6622e-03, -3.3703e-01,  ..., -1.1474e-01,\n",
       "          -7.3527e-01, -5.3881e-01],\n",
       "         [ 1.2157e+00, -3.2067e-01,  1.4775e-02,  ...,  3.4069e-02,\n",
       "           4.5909e-02, -8.2570e-01]],\n",
       "\n",
       "        [[ 1.0539e+00, -5.6921e-01,  2.0303e-01,  ...,  2.7515e-01,\n",
       "          -1.4765e+00, -4.8923e-01],\n",
       "         [ 1.6472e+00, -9.5447e-01,  5.1294e-01,  ..., -5.7168e-01,\n",
       "          -2.1548e+00, -7.7367e-01],\n",
       "         [ 2.8882e-01, -5.9791e-01,  5.6945e-01,  ..., -4.9382e-02,\n",
       "          -1.7232e+00, -2.4048e-01],\n",
       "         [ 7.0056e-01, -4.3437e-01,  3.4691e-01,  ..., -1.7854e-01,\n",
       "          -1.9971e+00, -3.8533e-01],\n",
       "         [ 1.3083e+00, -5.1270e-01,  4.9434e-01,  ...,  6.5615e-02,\n",
       "          -2.0005e+00, -1.4074e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2695, -0.2492, -0.0831,  ..., -0.0351,  0.7340,  0.1247],\n",
       "         [ 0.7348,  0.1523,  0.2606,  ..., -0.1404,  0.9398,  0.1040],\n",
       "         [ 0.9366, -0.1383,  0.6437,  ..., -0.0716,  0.6156,  0.0723],\n",
       "         [ 0.8001, -0.1691,  0.2462,  ...,  0.4305,  0.3796,  0.1382],\n",
       "         [ 0.6385, -0.2078,  0.6147,  ..., -0.0214,  0.7147,  0.2676]],\n",
       "\n",
       "        [[ 0.5024, -0.2017,  0.0671,  ..., -0.0914,  0.2362, -0.3528],\n",
       "         [ 0.4034, -0.4648,  0.6177,  ...,  0.2243,  0.1670, -0.0913],\n",
       "         [ 0.6191, -0.2262,  0.3571,  ...,  0.6194,  0.1060,  0.1200],\n",
       "         [ 0.1643, -0.3347,  0.1732,  ...,  0.1812,  0.1764, -0.1784],\n",
       "         [ 0.4976, -0.5537,  0.2065,  ...,  0.0263,  0.2702, -0.1939]],\n",
       "\n",
       "        [[ 0.2947, -0.5224,  0.7865,  ...,  0.1304, -0.0791, -0.1319],\n",
       "         [ 0.6163, -0.3831,  0.7307,  ...,  0.0494, -0.8643, -0.3173],\n",
       "         [-0.2894, -0.4522,  0.9227,  ...,  0.4364, -0.5582,  0.0109],\n",
       "         [ 0.2058, -0.2946,  0.7202,  ...,  0.1398, -0.5434, -0.1115],\n",
       "         [ 0.4102, -0.5085,  0.7268,  ...,  0.3014, -0.3204,  0.0106]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "acu_vis_dict = {0: torch.randn((bottleneck))}\n",
    "model(**model.dummy_inputs, acu_vis_dict=acu_vis_dict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59c55fd4b3978c0def058b870785763f0b3768b417004cdd166463757ca86d4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Multimodal_Schizo_adapter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
