{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoAdapterModel\n",
    "# from transformers import AutoModel\n",
    "from torchinfo import summary\n",
    "\n",
    "from transformers import (\n",
    "    ADAPTER_CONFIG_MAP,\n",
    "    ADAPTER_MODEL_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    AutoAdapterModel,\n",
    "    AutoTokenizer,\n",
    "    HoulsbyConfig,\n",
    "    HoulsbyInvConfig,\n",
    "    MAMConfig,\n",
    "    PfeifferConfig,\n",
    "    PfeifferInvConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"/home/chihyuan/Multimodal-Schizo/weights/chinese-electra-108g-small-discriminator_CTB8\",\n",
      "  \"_num_labels\": 2,\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {},\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"prediction_heads\": {},\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "None\n",
      "ElectraAdapterModel(\n",
      "  (shared_parameters): ModuleDict()\n",
      "  (electra): ElectraModel(\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict()\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (prefix_tuning): PrefixTuningPool(\n",
      "      (prefix_tunings): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict()\n",
      ")\n",
      "=========================================================================================================\n",
      "Layer (type (var_name):depth-idx)                                                Param #\n",
      "=========================================================================================================\n",
      "ElectraAdapterModel                                                              --\n",
      "├─ModuleDict (shared_parameters): 1-1                                            --\n",
      "├─ElectraModel (electra): 1-2                                                    --\n",
      "│    └─ModuleDict (shared_parameters): 2-1                                       --\n",
      "│    └─ModuleDict (invertible_adapters): 2-2                                     --\n",
      "│    └─ElectraEmbeddings (embeddings): 2-3                                       --\n",
      "│    │    └─Embedding (word_embeddings): 3-1                                     2,704,384\n",
      "│    │    └─Embedding (position_embeddings): 3-2                                 65,536\n",
      "│    │    └─Embedding (token_type_embeddings): 3-3                               256\n",
      "│    │    └─LayerNorm (LayerNorm): 3-4                                           256\n",
      "│    │    └─Dropout (dropout): 3-5                                               --\n",
      "│    └─Linear (embeddings_project): 2-4                                          33,024\n",
      "│    └─ElectraEncoder (encoder): 2-5                                             --\n",
      "│    │    └─ModuleList (layer): 3-6                                              --\n",
      "│    │    │    └─ElectraLayer (0): 4-1                                           --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-1                          --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-1                      --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-1                              65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-2                                65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-3                              65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-4                           --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-5            --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-1                --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-1       --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-2                       --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-6                              65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-7                       512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-8                           --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-9                       --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-10          --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-2                    --\n",
      "│    │    │    │    │    └─Linear (dense): 6-3                                   263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-4             --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-3                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-5                                   262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-6                            512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-7                                --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-8                            --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-9                --\n",
      "│    │    │    └─ElectraLayer (1): 4-2                                           --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-4                          --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-10                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-11                             65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-12                               65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-13                             65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-14                          --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-15           --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-2                --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-2       --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-11                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-16                             65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-17                      512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-18                          --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-19                      --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-20          --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-5                    --\n",
      "│    │    │    │    │    └─Linear (dense): 6-12                                  263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-13            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-6                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-14                                  262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-15                           512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-16                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-17                           --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-18               --\n",
      "│    │    │    └─ElectraLayer (2): 4-3                                           --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-7                          --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-19                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-21                             65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-22                               65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-23                             65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-24                          --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-25           --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-3                --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-3       --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-20                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-26                             65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-27                      512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-28                          --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-29                      --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-30          --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-8                    --\n",
      "│    │    │    │    │    └─Linear (dense): 6-21                                  263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-22            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-9                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-23                                  262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-24                           512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-25                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-26                           --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-27               --\n",
      "│    │    │    └─ElectraLayer (3): 4-4                                           --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-10                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-28                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-31                             65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-32                               65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-33                             65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-34                          --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-35           --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-4                --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-4       --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-29                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-36                             65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-37                      512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-38                          --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-39                      --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-40          --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-11                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-30                                  263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-31            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-12                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-32                                  262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-33                           512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-34                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-35                           --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-36               --\n",
      "│    │    │    └─ElectraLayer (4): 4-5                                           --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-13                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-37                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-41                             65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-42                               65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-43                             65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-44                          --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-45           --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-5                --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-5       --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-38                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-46                             65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-47                      512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-48                          --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-49                      --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-50          --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-14                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-39                                  263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-40            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-15                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-41                                  262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-42                           512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-43                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-44                           --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-45               --\n",
      "│    │    │    └─ElectraLayer (5): 4-6                                           --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-16                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-46                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-51                             65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-52                               65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-53                             65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-54                          --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-55           --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-6                --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-6       --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-47                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-56                             65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-57                      512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-58                          --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-59                      --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-60          --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-17                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-48                                  263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-49            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-18                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-50                                  262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-51                           512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-52                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-53                           --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-54               --\n",
      "│    │    │    └─ElectraLayer (6): 4-7                                           --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-19                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-55                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-61                             65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-62                               65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-63                             65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-64                          --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-65           --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-7                --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-7       --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-56                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-66                             65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-67                      512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-68                          --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-69                      --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-70          --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-20                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-57                                  263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-58            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-21                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-59                                  262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-60                           512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-61                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-62                           --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-63               --\n",
      "│    │    │    └─ElectraLayer (7): 4-8                                           --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-22                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-64                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-71                             65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-72                               65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-73                             65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-74                          --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-75           --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-8                --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-8       --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-65                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-76                             65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-77                      512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-78                          --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-79                      --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-80          --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-23                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-66                                  263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-67            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-24                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-68                                  262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-69                           512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-70                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-71                           --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-72               --\n",
      "│    │    │    └─ElectraLayer (8): 4-9                                           --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-25                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-73                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-81                             65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-82                               65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-83                             65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-84                          --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-85           --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-9                --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-9       --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-74                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-86                             65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-87                      512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-88                          --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-89                      --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-90          --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-26                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-75                                  263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-76            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-27                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-77                                  262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-78                           512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-79                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-80                           --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-81               --\n",
      "│    │    │    └─ElectraLayer (9): 4-10                                          --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-28                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-82                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-91                             65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-92                               65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-93                             65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-94                          --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-95           --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-10               --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-10      --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-83                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-96                             65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-97                      512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-98                          --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-99                      --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-100         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-29                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-84                                  263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-85            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-30                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-86                                  262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-87                           512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-88                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-89                           --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-90               --\n",
      "│    │    │    └─ElectraLayer (10): 4-11                                         --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-31                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-91                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-101                            65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-102                              65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-103                            65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-104                         --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-105          --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-11               --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-11      --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-92                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-106                            65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-107                     512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-108                         --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-109                     --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-110         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-32                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-93                                  263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-94            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-33                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-95                                  262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-96                           512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-97                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-98                           --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-99               --\n",
      "│    │    │    └─ElectraLayer (11): 4-12                                         --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-34                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-100                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-111                            65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-112                              65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-113                            65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-114                         --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-115          --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-12               --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-12      --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-101                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-116                            65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-117                     512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-118                         --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-119                     --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-120         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-35                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-102                                 263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-103           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-36                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-104                                 262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-105                          512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-106                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-107                          --\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-108              --\n",
      "│    └─PrefixTuningPool (prefix_tuning): 2-6                                     --\n",
      "│    │    └─ModuleDict (prefix_tunings): 3-7                                     --\n",
      "├─ModuleDict (heads): 1-3                                                        --\n",
      "=========================================================================================================\n",
      "Total params: 12,280,576\n",
      "Trainable params: 12,280,576\n",
      "Non-trainable params: 0\n",
      "=========================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModel.from_pretrained(\n",
    "model = AutoAdapterModel.from_pretrained(\n",
    "    \"/home/chihyuan/Multimodal-Schizo/weights/chinese-electra-108g-small-discriminator_CTB8\",\n",
    "    # \"bert-base-uncased\",\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "print(model.config)\n",
    "print(model.active_adapters)\n",
    "print(model)\n",
    "print(summary(model, depth=10, row_settings=[\"depth\", \"var_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass psuedo input to see output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input_ids = torch.randint(high=512, size=(2, 20))\n",
    "output = model(input_ids)\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add default adapters (pfeiffer) and make only adapters trainable (activate adapters to be used in forward as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"/home/chihyuan/Multimodal-Schizo/weights/chinese-electra-108g-small-discriminator_CTB8\",\n",
      "  \"_num_labels\": 2,\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {\n",
      "      \"syn\": \"pfeiffer\"\n",
      "    },\n",
      "    \"config_map\": {},\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"prediction_heads\": {},\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Stack[syn]\n",
      "ElectraAdapterModel(\n",
      "  (shared_parameters): ModuleDict()\n",
      "  (electra): ElectraModel(\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (prefix_tuning): PrefixTuningPool(\n",
      "      (prefix_tunings): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict()\n",
      ")\n",
      "===================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                                                          Param #\n",
      "===================================================================================================================\n",
      "ElectraAdapterModel                                                                        --\n",
      "├─ModuleDict (shared_parameters): 1-1                                                      --\n",
      "├─ElectraModel (electra): 1-2                                                              --\n",
      "│    └─ModuleDict (shared_parameters): 2-1                                                 --\n",
      "│    └─ModuleDict (invertible_adapters): 2-2                                               --\n",
      "│    └─ElectraEmbeddings (embeddings): 2-3                                                 --\n",
      "│    │    └─Embedding (word_embeddings): 3-1                                               (2,704,384)\n",
      "│    │    └─Embedding (position_embeddings): 3-2                                           (65,536)\n",
      "│    │    └─Embedding (token_type_embeddings): 3-3                                         (256)\n",
      "│    │    └─LayerNorm (LayerNorm): 3-4                                                     (256)\n",
      "│    │    └─Dropout (dropout): 3-5                                                         --\n",
      "│    └─Linear (embeddings_project): 2-4                                                    (33,024)\n",
      "│    └─ElectraEncoder (encoder): 2-5                                                       --\n",
      "│    │    └─ModuleList (layer): 3-6                                                        --\n",
      "│    │    │    └─ElectraLayer (0): 4-1                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-1                                    --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-1                                --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-1                                        (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-2                                          (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-3                                        (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-4                                     --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-5                      --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-1                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-1                 --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-2                                 --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-6                                        (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-7                                 (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-8                                     --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-9                                 --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-10                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-2                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-3                                             (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-4                       --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-3                                          --\n",
      "│    │    │    │    │    └─Linear (dense): 6-5                                             (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-6                                      (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-7                                          --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-8                                      --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-11                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-2        --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-2                                    --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-3                        --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-3                                  4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-4               --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-1                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-4                              4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-9                          --\n",
      "│    │    │    └─ElectraLayer (1): 4-2                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-4                                    --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-10                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-12                                       (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-13                                         (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-14                                       (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-15                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-16                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-5                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-5                 --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-11                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-17                                       (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-18                                (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-19                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-20                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-21                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-5                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-12                                            (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-13                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-6                                          --\n",
      "│    │    │    │    │    └─Linear (dense): 6-14                                            (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-15                                     (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-16                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-17                                     --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-22                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-6        --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-6                                    --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-7                        --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-7                                  4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-8               --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-2                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-8                              4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-18                         --\n",
      "│    │    │    └─ElectraLayer (2): 4-3                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-7                                    --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-19                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-23                                       (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-24                                         (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-25                                       (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-26                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-27                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-9                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-9                 --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-20                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-28                                       (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-29                                (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-30                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-31                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-32                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-8                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-21                                            (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-22                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-9                                          --\n",
      "│    │    │    │    │    └─Linear (dense): 6-23                                            (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-24                                     (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-25                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-26                                     --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-33                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-10       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-10                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-11                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-11                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-12              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-3                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-12                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-27                         --\n",
      "│    │    │    └─ElectraLayer (3): 4-4                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-10                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-28                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-34                                       (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-35                                         (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-36                                       (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-37                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-38                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-13                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-13                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-29                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-39                                       (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-40                                (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-41                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-42                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-43                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-11                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-30                                            (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-31                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-12                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-32                                            (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-33                                     (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-34                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-35                                     --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-44                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-14       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-14                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-15                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-15                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-16              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-4                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-16                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-36                         --\n",
      "│    │    │    └─ElectraLayer (4): 4-5                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-13                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-37                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-45                                       (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-46                                         (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-47                                       (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-48                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-49                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-17                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-17                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-38                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-50                                       (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-51                                (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-52                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-53                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-54                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-14                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-39                                            (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-40                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-15                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-41                                            (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-42                                     (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-43                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-44                                     --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-55                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-18       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-18                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-19                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-19                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-20              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-5                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-20                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-45                         --\n",
      "│    │    │    └─ElectraLayer (5): 4-6                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-16                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-46                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-56                                       (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-57                                         (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-58                                       (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-59                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-60                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-21                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-21                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-47                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-61                                       (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-62                                (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-63                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-64                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-65                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-17                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-48                                            (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-49                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-18                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-50                                            (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-51                                     (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-52                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-53                                     --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-66                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-22       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-22                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-23                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-23                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-24              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-6                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-24                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-54                         --\n",
      "│    │    │    └─ElectraLayer (6): 4-7                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-19                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-55                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-67                                       (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-68                                         (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-69                                       (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-70                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-71                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-25                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-25                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-56                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-72                                       (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-73                                (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-74                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-75                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-76                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-20                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-57                                            (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-58                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-21                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-59                                            (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-60                                     (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-61                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-62                                     --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-77                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-26       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-26                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-27                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-27                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-28              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-7                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-28                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-63                         --\n",
      "│    │    │    └─ElectraLayer (7): 4-8                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-22                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-64                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-78                                       (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-79                                         (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-80                                       (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-81                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-82                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-29                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-29                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-65                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-83                                       (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-84                                (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-85                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-86                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-87                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-23                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-66                                            (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-67                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-24                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-68                                            (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-69                                     (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-70                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-71                                     --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-88                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-30       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-30                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-31                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-31                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-32              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-8                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-32                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-72                         --\n",
      "│    │    │    └─ElectraLayer (8): 4-9                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-25                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-73                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-89                                       (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-90                                         (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-91                                       (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-92                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-93                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-33                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-33                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-74                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-94                                       (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-95                                (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-96                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-97                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-98                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-26                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-75                                            (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-76                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-27                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-77                                            (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-78                                     (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-79                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-80                                     --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-99                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-34       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-34                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-35                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-35                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-36              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-9                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-36                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-81                         --\n",
      "│    │    │    └─ElectraLayer (9): 4-10                                                    --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-28                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-82                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-100                                      (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-101                                        (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-102                                      (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-103                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-104                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-37                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-37                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-83                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-105                                      (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-106                               (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-107                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-108                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-109                   --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-29                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-84                                            (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-85                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-30                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-86                                            (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-87                                     (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-88                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-89                                     --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-110                                       --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-38       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-38                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-39                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-39                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-40              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-10                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-40                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-90                         --\n",
      "│    │    │    └─ElectraLayer (10): 4-11                                                   --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-31                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-91                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-111                                      (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-112                                        (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-113                                      (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-114                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-115                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-41                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-41                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-92                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-116                                      (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-117                               (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-118                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-119                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-120                   --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-32                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-93                                            (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-94                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-33                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-95                                            (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-96                                     (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-97                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-98                                     --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-121                                       --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-42       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-42                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-43                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-43                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-44              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-11                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-44                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-99                         --\n",
      "│    │    │    └─ElectraLayer (11): 4-12                                                   --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-34                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-100                              --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-122                                      (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-123                                        (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-124                                      (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-125                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-126                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-45                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-45                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-101                               --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-127                                      (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-128                               (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-129                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-130                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-131                   --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-35                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-102                                           (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-103                     --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-36                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-104                                           (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-105                                    (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-106                                        --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-107                                    --\n",
      "│    │    │    │    │    │    └─Adapter (syn): 7-132                                       --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-46       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-46                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-47                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-47                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-48              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-12                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-48                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-108                        --\n",
      "│    └─PrefixTuningPool (prefix_tuning): 2-6                                               --\n",
      "│    │    └─ModuleDict (prefix_tunings): 3-7                                               --\n",
      "├─ModuleDict (heads): 1-3                                                                  --\n",
      "===================================================================================================================\n",
      "Total params: 12,382,144\n",
      "Trainable params: 101,568\n",
      "Non-trainable params: 12,280,576\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model.add_adapter(\"syn\")\n",
    "model.train_adapter(\"syn\")\n",
    "\n",
    "print(model.config)\n",
    "print(model.active_adapters)\n",
    "print(model)\n",
    "print(summary(model, depth=10, row_settings=[\"depth\", \"var_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete adapters and make all param trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {\n",
      "      \"pfeiffer_inv\": \"9ed5b5a29de19b71\"\n",
      "    },\n",
      "    \"config_map\": {\n",
      "      \"9ed5b5a29de19b71\": {\n",
      "        \"adapter_residual_before_ln\": false,\n",
      "        \"cross_adapter\": false,\n",
      "        \"factorized_phm_W\": true,\n",
      "        \"factorized_phm_rule\": false,\n",
      "        \"hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
      "        \"init_weights\": \"bert\",\n",
      "        \"inv_adapter\": \"nice\",\n",
      "        \"inv_adapter_reduction_factor\": 2,\n",
      "        \"is_parallel\": false,\n",
      "        \"learn_phm\": true,\n",
      "        \"leave_out\": [],\n",
      "        \"ln_after\": false,\n",
      "        \"ln_before\": false,\n",
      "        \"mh_adapter\": false,\n",
      "        \"non_linearity\": \"relu\",\n",
      "        \"original_ln_after\": true,\n",
      "        \"original_ln_before\": true,\n",
      "        \"output_adapter\": true,\n",
      "        \"phm_bias\": true,\n",
      "        \"phm_c_init\": \"normal\",\n",
      "        \"phm_dim\": 4,\n",
      "        \"phm_init_range\": 0.0001,\n",
      "        \"phm_layer\": false,\n",
      "        \"phm_rank\": 1,\n",
      "        \"reduction_factor\": 16,\n",
      "        \"residual_before_ln\": true,\n",
      "        \"scaling\": 1.0,\n",
      "        \"shared_W_phm\": false,\n",
      "        \"shared_phm_rule\": true\n",
      "      }\n",
      "    },\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"prediction_heads\": {\n",
      "    \"pfeiffer_inv\": {\n",
      "      \"activation_function\": \"tanh\",\n",
      "      \"bias\": true,\n",
      "      \"head_type\": \"classification\",\n",
      "      \"label2id\": {\n",
      "        \"LABEL_0\": 0,\n",
      "        \"LABEL_1\": 1\n",
      "      },\n",
      "      \"layers\": 2,\n",
      "      \"num_labels\": 2,\n",
      "      \"use_pooler\": false\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Stack[pfeiffer_inv]\n",
      "BertAdapterModel(\n",
      "  (shared_parameters): ModuleDict()\n",
      "  (bert): BertModel(\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (invertible_adapters): ModuleDict(\n",
      "      (pfeiffer_inv): NICECouplingBlock(\n",
      "        (F): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=192, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "          (2): Linear(in_features=192, out_features=384, bias=True)\n",
      "        )\n",
      "        (G): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=192, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "          (2): Linear(in_features=192, out_features=384, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=768, out_features=48, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (prefix_tuning): PrefixTuningPool(\n",
      "      (prefix_tunings): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (pfeiffer_inv): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (2): Activation_Function_Class(\n",
      "        (f): Tanh()\n",
      "      )\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "===================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                                                          Param #\n",
      "===================================================================================================================\n",
      "BertAdapterModel                                                                           --\n",
      "├─ModuleDict (shared_parameters): 1-1                                                      --\n",
      "├─BertModel (bert): 1-2                                                                    --\n",
      "│    └─ModuleDict (shared_parameters): 2-1                                                 --\n",
      "│    └─ModuleDict (invertible_adapters): 2-2                                               --\n",
      "│    │    └─NICECouplingBlock (pfeiffer_inv): 3-1                                          --\n",
      "│    │    │    └─Sequential (F): 4-1                                                       --\n",
      "│    │    │    │    └─Linear (0): 5-1                                                      73,920\n",
      "│    │    │    │    └─Activation_Function_Class (1): 5-2                                   --\n",
      "│    │    │    │    │    └─ReLU (f): 6-1                                                   --\n",
      "│    │    │    │    └─Linear (2): 5-3                                                      74,112\n",
      "│    │    │    └─Sequential (G): 4-2                                                       --\n",
      "│    │    │    │    └─Linear (0): 5-4                                                      73,920\n",
      "│    │    │    │    └─Activation_Function_Class (1): 5-5                                   --\n",
      "│    │    │    │    │    └─ReLU (f): 6-2                                                   --\n",
      "│    │    │    │    └─Linear (2): 5-6                                                      74,112\n",
      "│    └─BertEmbeddings (embeddings): 2-3                                                    --\n",
      "│    │    └─Embedding (word_embeddings): 3-2                                               23,440,896\n",
      "│    │    └─Embedding (position_embeddings): 3-3                                           393,216\n",
      "│    │    └─Embedding (token_type_embeddings): 3-4                                         1,536\n",
      "│    │    └─LayerNorm (LayerNorm): 3-5                                                     1,536\n",
      "│    │    └─Dropout (dropout): 3-6                                                         --\n",
      "│    └─BertEncoder (encoder): 2-4                                                          --\n",
      "│    │    └─ModuleList (layer): 3-7                                                        --\n",
      "│    │    │    └─BertLayer (0): 4-3                                                        --\n",
      "│    │    │    │    └─BertAttention (attention): 5-7                                       --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-3                                   --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-1                                        590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-2                                          590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-3                                        590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-4                                     --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-5                      --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-1                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-1                 --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-4                                    --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-6                                        590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-7                                 1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-8                                     --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-9                                 --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-10                    --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-8                                 --\n",
      "│    │    │    │    │    └─Linear (dense): 6-5                                             2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-6                       --\n",
      "│    │    │    │    └─BertOutput (output): 5-9                                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-7                                             2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-8                                      1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-9                                          --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-10                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-11                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-2        --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-2                                    --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-3                        --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-3                                  36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-4               --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-1                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-4                              37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-11                         --\n",
      "│    │    │    └─BertLayer (1): 4-4                                                        --\n",
      "│    │    │    │    └─BertAttention (attention): 5-10                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-12                                  --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-12                                       590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-13                                         590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-14                                       590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-15                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-16                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-5                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-5                 --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-13                                   --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-17                                       590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-18                                1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-19                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-20                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-21                    --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-11                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-14                                            2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-15                      --\n",
      "│    │    │    │    └─BertOutput (output): 5-12                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-16                                            2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-17                                     1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-18                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-19                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-22                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-6        --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-6                                    --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-7                        --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-7                                  36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-8               --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-2                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-8                              37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-20                         --\n",
      "│    │    │    └─BertLayer (2): 4-5                                                        --\n",
      "│    │    │    │    └─BertAttention (attention): 5-13                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-21                                  --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-23                                       590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-24                                         590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-25                                       590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-26                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-27                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-9                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-9                 --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-22                                   --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-28                                       590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-29                                1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-30                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-31                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-32                    --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-14                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-23                                            2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-24                      --\n",
      "│    │    │    │    └─BertOutput (output): 5-15                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-25                                            2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-26                                     1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-27                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-28                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-33                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-10       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-10                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-11                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-11                                 36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-12              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-3                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-12                             37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-29                         --\n",
      "│    │    │    └─BertLayer (3): 4-6                                                        --\n",
      "│    │    │    │    └─BertAttention (attention): 5-16                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-30                                  --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-34                                       590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-35                                         590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-36                                       590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-37                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-38                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-13                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-13                --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-31                                   --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-39                                       590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-40                                1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-41                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-42                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-43                    --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-17                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-32                                            2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-33                      --\n",
      "│    │    │    │    └─BertOutput (output): 5-18                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-34                                            2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-35                                     1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-36                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-37                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-44                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-14       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-14                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-15                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-15                                 36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-16              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-4                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-16                             37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-38                         --\n",
      "│    │    │    └─BertLayer (4): 4-7                                                        --\n",
      "│    │    │    │    └─BertAttention (attention): 5-19                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-39                                  --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-45                                       590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-46                                         590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-47                                       590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-48                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-49                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-17                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-17                --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-40                                   --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-50                                       590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-51                                1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-52                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-53                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-54                    --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-20                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-41                                            2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-42                      --\n",
      "│    │    │    │    └─BertOutput (output): 5-21                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-43                                            2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-44                                     1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-45                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-46                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-55                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-18       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-18                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-19                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-19                                 36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-20              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-5                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-20                             37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-47                         --\n",
      "│    │    │    └─BertLayer (5): 4-8                                                        --\n",
      "│    │    │    │    └─BertAttention (attention): 5-22                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-48                                  --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-56                                       590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-57                                         590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-58                                       590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-59                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-60                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-21                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-21                --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-49                                   --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-61                                       590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-62                                1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-63                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-64                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-65                    --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-23                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-50                                            2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-51                      --\n",
      "│    │    │    │    └─BertOutput (output): 5-24                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-52                                            2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-53                                     1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-54                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-55                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-66                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-22       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-22                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-23                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-23                                 36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-24              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-6                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-24                             37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-56                         --\n",
      "│    │    │    └─BertLayer (6): 4-9                                                        --\n",
      "│    │    │    │    └─BertAttention (attention): 5-25                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-57                                  --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-67                                       590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-68                                         590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-69                                       590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-70                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-71                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-25                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-25                --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-58                                   --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-72                                       590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-73                                1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-74                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-75                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-76                    --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-26                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-59                                            2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-60                      --\n",
      "│    │    │    │    └─BertOutput (output): 5-27                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-61                                            2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-62                                     1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-63                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-64                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-77                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-26       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-26                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-27                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-27                                 36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-28              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-7                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-28                             37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-65                         --\n",
      "│    │    │    └─BertLayer (7): 4-10                                                       --\n",
      "│    │    │    │    └─BertAttention (attention): 5-28                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-66                                  --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-78                                       590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-79                                         590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-80                                       590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-81                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-82                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-29                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-29                --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-67                                   --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-83                                       590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-84                                1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-85                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-86                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-87                    --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-29                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-68                                            2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-69                      --\n",
      "│    │    │    │    └─BertOutput (output): 5-30                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-70                                            2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-71                                     1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-72                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-73                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-88                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-30       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-30                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-31                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-31                                 36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-32              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-8                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-32                             37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-74                         --\n",
      "│    │    │    └─BertLayer (8): 4-11                                                       --\n",
      "│    │    │    │    └─BertAttention (attention): 5-31                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-75                                  --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-89                                       590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-90                                         590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-91                                       590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-92                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-93                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-33                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-33                --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-76                                   --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-94                                       590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-95                                1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-96                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-97                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-98                    --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-32                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-77                                            2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-78                      --\n",
      "│    │    │    │    └─BertOutput (output): 5-33                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-79                                            2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-80                                     1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-81                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-82                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-99                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-34       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-34                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-35                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-35                                 36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-36              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-9                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-36                             37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-83                         --\n",
      "│    │    │    └─BertLayer (9): 4-12                                                       --\n",
      "│    │    │    │    └─BertAttention (attention): 5-34                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-84                                  --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-100                                      590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-101                                        590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-102                                      590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-103                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-104                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-37                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-37                --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-85                                   --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-105                                      590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-106                               1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-107                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-108                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-109                   --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-35                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-86                                            2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-87                      --\n",
      "│    │    │    │    └─BertOutput (output): 5-36                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-88                                            2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-89                                     1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-90                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-91                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-110                              --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-38       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-38                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-39                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-39                                 36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-40              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-10                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-40                             37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-92                         --\n",
      "│    │    │    └─BertLayer (10): 4-13                                                      --\n",
      "│    │    │    │    └─BertAttention (attention): 5-37                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-93                                  --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-111                                      590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-112                                        590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-113                                      590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-114                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-115                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-41                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-41                --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-94                                   --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-116                                      590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-117                               1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-118                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-119                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-120                   --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-38                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-95                                            2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-96                      --\n",
      "│    │    │    │    └─BertOutput (output): 5-39                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-97                                            2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-98                                     1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-99                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-100                                    --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-121                              --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-42       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-42                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-43                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-43                                 36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-44              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-11                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-44                             37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-101                        --\n",
      "│    │    │    └─BertLayer (11): 4-14                                                      --\n",
      "│    │    │    │    └─BertAttention (attention): 5-40                                      --\n",
      "│    │    │    │    │    └─BertSelfAttention (self): 6-102                                 --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-122                                      590,592\n",
      "│    │    │    │    │    │    └─Linear (key): 7-123                                        590,592\n",
      "│    │    │    │    │    │    └─Linear (value): 7-124                                      590,592\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-125                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-126                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-45                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-45                --\n",
      "│    │    │    │    │    └─BertSelfOutput (output): 6-103                                  --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-127                                      590,592\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-128                               1,536\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-129                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-130                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-131                   --\n",
      "│    │    │    │    └─BertIntermediate (intermediate): 5-41                                --\n",
      "│    │    │    │    │    └─Linear (dense): 6-104                                           2,362,368\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-105                     --\n",
      "│    │    │    │    └─BertOutput (output): 5-42                                            --\n",
      "│    │    │    │    │    └─Linear (dense): 6-106                                           2,360,064\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-107                                    1,536\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-108                                        --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-109                                    --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-132                              --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-46       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-46                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-47                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-47                                 36,912\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-48              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-12                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-48                             37,632\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-110                        --\n",
      "│    └─BertPooler (pooler): 2-5                                                            --\n",
      "│    │    └─Linear (dense): 3-8                                                            590,592\n",
      "│    │    └─Tanh (activation): 3-9                                                         --\n",
      "│    └─PrefixTuningPool (prefix_tuning): 2-6                                               --\n",
      "│    │    └─ModuleDict (prefix_tunings): 3-10                                              --\n",
      "├─ModuleDict (heads): 1-3                                                                  --\n",
      "│    └─ClassificationHead (pfeiffer_inv): 2-7                                              --\n",
      "│    │    └─Dropout (0): 3-11                                                              --\n",
      "│    │    └─Linear (1): 3-12                                                               590,592\n",
      "│    │    └─Activation_Function_Class (2): 3-13                                            --\n",
      "│    │    │    └─Tanh (f): 4-15                                                            --\n",
      "│    │    └─Dropout (3): 3-14                                                              --\n",
      "│    │    └─Linear (4): 3-15                                                               1,538\n",
      "===================================================================================================================\n",
      "Total params: 111,264,962\n",
      "Trainable params: 111,264,962\n",
      "Non-trainable params: 0\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model.delete_adapter(\"syn\")\n",
    "model.freeze_model(False)\n",
    "\n",
    "print(model.config)\n",
    "print(model.active_adapters)\n",
    "print(model)\n",
    "print(summary(model, depth=10, row_settings=[\"depth\", \"var_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add pre-defined adapter (houlsby) and make only adapters trainable (activate adapters to be used in forward as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"/home/chihyuan/Multimodal-Schizo/weights/chinese-electra-108g-small-discriminator_CTB8\",\n",
      "  \"_num_labels\": 2,\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {\n",
      "      \"new_syn\": \"b1017368d7a97b11\"\n",
      "    },\n",
      "    \"config_map\": {\n",
      "      \"b1017368d7a97b11\": {\n",
      "        \"adapter_residual_before_ln\": false,\n",
      "        \"cross_adapter\": false,\n",
      "        \"factorized_phm_W\": true,\n",
      "        \"factorized_phm_rule\": false,\n",
      "        \"hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
      "        \"init_weights\": \"bert\",\n",
      "        \"inv_adapter\": null,\n",
      "        \"inv_adapter_reduction_factor\": null,\n",
      "        \"is_parallel\": false,\n",
      "        \"learn_phm\": true,\n",
      "        \"leave_out\": [],\n",
      "        \"ln_after\": false,\n",
      "        \"ln_before\": false,\n",
      "        \"mh_adapter\": true,\n",
      "        \"non_linearity\": \"swish\",\n",
      "        \"original_ln_after\": true,\n",
      "        \"original_ln_before\": false,\n",
      "        \"output_adapter\": true,\n",
      "        \"phm_bias\": true,\n",
      "        \"phm_c_init\": \"normal\",\n",
      "        \"phm_dim\": 4,\n",
      "        \"phm_init_range\": 0.0001,\n",
      "        \"phm_layer\": false,\n",
      "        \"phm_rank\": 1,\n",
      "        \"reduction_factor\": 16,\n",
      "        \"residual_before_ln\": true,\n",
      "        \"scaling\": 1.0,\n",
      "        \"shared_W_phm\": false,\n",
      "        \"shared_phm_rule\": true\n",
      "      }\n",
      "    },\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"prediction_heads\": {},\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Stack[new_syn]\n",
      "ElectraAdapterModel(\n",
      "  (shared_parameters): ModuleDict()\n",
      "  (electra): ElectraModel(\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (invertible_adapters): ModuleDict()\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict(\n",
      "                (new_syn): Adapter(\n",
      "                  (non_linearity): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                  (adapter_down): Sequential(\n",
      "                    (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                    (1): Activation_Function_Class(\n",
      "                      (f): SiLUActivation()\n",
      "                    )\n",
      "                  )\n",
      "                  (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "                )\n",
      "              )\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (new_syn): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): SiLUActivation()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): SiLUActivation()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (prefix_tuning): PrefixTuningPool(\n",
      "      (prefix_tunings): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict()\n",
      ")\n",
      "========================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                                                               Param #\n",
      "========================================================================================================================\n",
      "ElectraAdapterModel                                                                             --\n",
      "├─ModuleDict (shared_parameters): 1-1                                                           --\n",
      "├─ElectraModel (electra): 1-2                                                                   --\n",
      "│    └─ModuleDict (shared_parameters): 2-1                                                      --\n",
      "│    └─ModuleDict (invertible_adapters): 2-2                                                    --\n",
      "│    └─ElectraEmbeddings (embeddings): 2-3                                                      --\n",
      "│    │    └─Embedding (word_embeddings): 3-1                                                    (2,704,384)\n",
      "│    │    └─Embedding (position_embeddings): 3-2                                                (65,536)\n",
      "│    │    └─Embedding (token_type_embeddings): 3-3                                              (256)\n",
      "│    │    └─LayerNorm (LayerNorm): 3-4                                                          (256)\n",
      "│    │    └─Dropout (dropout): 3-5                                                              --\n",
      "│    └─Linear (embeddings_project): 2-4                                                         (33,024)\n",
      "│    └─ElectraEncoder (encoder): 2-5                                                            --\n",
      "│    │    └─ModuleList (layer): 3-6                                                             --\n",
      "│    │    │    └─ElectraLayer (0): 4-1                                                          --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-1                                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-1                                     --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-1                                             (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-2                                               (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-3                                             (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-4                                          --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-5                           --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-1                               --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-1                      --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-2                                      --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-6                                             (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-7                                      (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-8                                          --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-9                                      --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-2                                     --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-2        --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-1                         --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-3                        --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-2                                 4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-3              --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-4                              4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-10                         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-2                                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-3                                                  (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-4                            --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-3                                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-5                                                  (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-6                                           (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-7                                               --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-8                                           --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-11                                         --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-3             --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-5                               --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-4                             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-6                                       4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-7                    --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-4                         --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-5                                   4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-9                               --\n",
      "│    │    │    └─ElectraLayer (1): 4-2                                                          --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-4                                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-10                                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-12                                            (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-13                                              (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-14                                            (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-15                                         --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-16                          --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-6                               --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-8                      --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-11                                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-17                                            (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-18                                     (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-19                                         --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-20                                     --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-7                                     --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-9        --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-5                         --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-10                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-6                                 4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-7              --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-11                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-21                         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-5                                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-12                                                 (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-13                           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-6                                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-14                                                 (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-15                                          (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-16                                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-17                                          --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-22                                         --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-8             --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-12                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-9                             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-13                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-14                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-8                         --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-10                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-18                              --\n",
      "│    │    │    └─ElectraLayer (2): 4-3                                                          --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-7                                         --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-19                                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-23                                            (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-24                                              (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-25                                            (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-26                                         --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-27                          --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-11                              --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-15                     --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-20                                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-28                                            (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-29                                     (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-30                                         --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-31                                     --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-12                                    --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-16       --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-9                         --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-17                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-10                                4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-11             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-18                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-32                         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-8                                   --\n",
      "│    │    │    │    │    └─Linear (dense): 6-21                                                 (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-22                           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-9                                               --\n",
      "│    │    │    │    │    └─Linear (dense): 6-23                                                 (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-24                                          (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-25                                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-26                                          --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-33                                         --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-13            --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-19                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-14                            --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-20                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-21                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-12                        --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-15                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-27                              --\n",
      "│    │    │    └─ElectraLayer (3): 4-4                                                          --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-10                                        --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-28                                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-34                                            (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-35                                              (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-36                                            (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-37                                         --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-38                          --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-16                              --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-22                     --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-29                                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-39                                            (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-40                                     (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-41                                         --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-42                                     --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-17                                    --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-23       --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-13                        --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-24                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-14                                4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-15             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-25                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-43                         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-11                                  --\n",
      "│    │    │    │    │    └─Linear (dense): 6-30                                                 (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-31                           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-12                                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-32                                                 (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-33                                          (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-34                                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-35                                          --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-44                                         --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-18            --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-26                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-19                            --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-27                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-28                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-16                        --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-20                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-36                              --\n",
      "│    │    │    └─ElectraLayer (4): 4-5                                                          --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-13                                        --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-37                                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-45                                            (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-46                                              (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-47                                            (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-48                                         --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-49                          --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-21                              --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-29                     --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-38                                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-50                                            (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-51                                     (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-52                                         --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-53                                     --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-22                                    --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-30       --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-17                        --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-31                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-18                                4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-19             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-32                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-54                         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-14                                  --\n",
      "│    │    │    │    │    └─Linear (dense): 6-39                                                 (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-40                           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-15                                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-41                                                 (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-42                                          (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-43                                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-44                                          --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-55                                         --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-23            --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-33                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-24                            --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-34                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-35                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-20                        --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-25                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-45                              --\n",
      "│    │    │    └─ElectraLayer (5): 4-6                                                          --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-16                                        --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-46                                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-56                                            (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-57                                              (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-58                                            (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-59                                         --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-60                          --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-26                              --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-36                     --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-47                                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-61                                            (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-62                                     (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-63                                         --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-64                                     --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-27                                    --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-37       --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-21                        --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-38                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-22                                4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-23             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-39                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-65                         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-17                                  --\n",
      "│    │    │    │    │    └─Linear (dense): 6-48                                                 (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-49                           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-18                                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-50                                                 (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-51                                          (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-52                                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-53                                          --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-66                                         --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-28            --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-40                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-29                            --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-41                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-42                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-24                        --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-30                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-54                              --\n",
      "│    │    │    └─ElectraLayer (6): 4-7                                                          --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-19                                        --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-55                                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-67                                            (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-68                                              (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-69                                            (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-70                                         --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-71                          --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-31                              --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-43                     --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-56                                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-72                                            (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-73                                     (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-74                                         --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-75                                     --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-32                                    --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-44       --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-25                        --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-45                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-26                                4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-27             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-46                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-76                         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-20                                  --\n",
      "│    │    │    │    │    └─Linear (dense): 6-57                                                 (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-58                           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-21                                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-59                                                 (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-60                                          (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-61                                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-62                                          --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-77                                         --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-33            --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-47                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-34                            --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-48                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-49                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-28                        --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-35                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-63                              --\n",
      "│    │    │    └─ElectraLayer (7): 4-8                                                          --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-22                                        --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-64                                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-78                                            (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-79                                              (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-80                                            (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-81                                         --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-82                          --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-36                              --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-50                     --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-65                                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-83                                            (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-84                                     (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-85                                         --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-86                                     --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-37                                    --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-51       --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-29                        --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-52                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-30                                4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-31             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-53                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-87                         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-23                                  --\n",
      "│    │    │    │    │    └─Linear (dense): 6-66                                                 (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-67                           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-24                                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-68                                                 (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-69                                          (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-70                                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-71                                          --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-88                                         --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-38            --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-54                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-39                            --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-55                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-56                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-32                        --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-40                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-72                              --\n",
      "│    │    │    └─ElectraLayer (8): 4-9                                                          --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-25                                        --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-73                                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-89                                            (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-90                                              (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-91                                            (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-92                                         --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-93                          --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-41                              --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-57                     --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-74                                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-94                                            (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-95                                     (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-96                                         --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-97                                     --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-42                                    --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-58       --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-33                        --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-59                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-34                                4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-35             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-60                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-98                         --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-26                                  --\n",
      "│    │    │    │    │    └─Linear (dense): 6-75                                                 (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-76                           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-27                                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-77                                                 (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-78                                          (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-79                                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-80                                          --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-99                                         --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-43            --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-61                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-44                            --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-62                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-63                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-36                        --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-45                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-81                              --\n",
      "│    │    │    └─ElectraLayer (9): 4-10                                                         --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-28                                        --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-82                                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-100                                           (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-101                                             (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-102                                           (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-103                                        --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-104                         --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-46                              --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-64                     --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-83                                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-105                                           (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-106                                    (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-107                                        --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-108                                    --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-47                                    --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-65       --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-37                        --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-66                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-38                                4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-39             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-67                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-109                        --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-29                                  --\n",
      "│    │    │    │    │    └─Linear (dense): 6-84                                                 (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-85                           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-30                                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-86                                                 (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-87                                          (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-88                                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-89                                          --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-110                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-48            --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-68                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-49                            --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-69                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-70                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-40                        --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-50                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-90                              --\n",
      "│    │    │    └─ElectraLayer (10): 4-11                                                        --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-31                                        --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-91                                    --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-111                                           (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-112                                             (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-113                                           (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-114                                        --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-115                         --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-51                              --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-71                     --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-92                                     --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-116                                           (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-117                                    (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-118                                        --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-119                                    --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-52                                    --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-72       --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-41                        --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-73                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-42                                4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-43             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-74                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-120                        --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-32                                  --\n",
      "│    │    │    │    │    └─Linear (dense): 6-93                                                 (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-94                           --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-33                                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-95                                                 (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-96                                          (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-97                                              --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-98                                          --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-121                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-53            --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-75                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-54                            --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-76                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-77                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-44                        --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-55                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-99                              --\n",
      "│    │    │    └─ElectraLayer (11): 4-12                                                        --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-34                                        --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-100                                   --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-122                                           (65,792)\n",
      "│    │    │    │    │    │    └─Linear (key): 7-123                                             (65,792)\n",
      "│    │    │    │    │    │    └─Linear (value): 7-124                                           (65,792)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-125                                        --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-126                         --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-56                              --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-78                     --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-101                                    --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-127                                           (65,792)\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-128                                    (512)\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-129                                        --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-130                                    --\n",
      "│    │    │    │    │    │    │    └─Adapter (new_syn): 8-57                                    --\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 9-79       --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-45                        --\n",
      "│    │    │    │    │    │    │    │    └─Sequential (adapter_down): 9-80                       --\n",
      "│    │    │    │    │    │    │    │    │    └─Linear (0): 10-46                                4,112\n",
      "│    │    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 10-47             --\n",
      "│    │    │    │    │    │    │    │    └─Linear (adapter_up): 9-81                             4,352\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-131                        --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-35                                  --\n",
      "│    │    │    │    │    └─Linear (dense): 6-102                                                (263,168)\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-103                          --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-36                                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-104                                                (262,400)\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-105                                         (512)\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-106                                             --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-107                                         --\n",
      "│    │    │    │    │    │    └─Adapter (new_syn): 7-132                                        --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-58            --\n",
      "│    │    │    │    │    │    │    │    └─SiLUActivation (f): 9-82                              --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-59                            --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-83                                      4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-84                   --\n",
      "│    │    │    │    │    │    │    │    │    └─SiLUActivation (f): 10-48                        --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-60                                  4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-108                             --\n",
      "│    └─PrefixTuningPool (prefix_tuning): 2-6                                                    --\n",
      "│    │    └─ModuleDict (prefix_tunings): 3-7                                                    --\n",
      "├─ModuleDict (heads): 1-3                                                                       --\n",
      "========================================================================================================================\n",
      "Total params: 12,483,712\n",
      "Trainable params: 203,136\n",
      "Non-trainable params: 12,280,576\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import HoulsbyConfig  \n",
    "# check ADAPTER_CONFIG_MAP in src/transformers/adapters/configuration.py for pre-defined adapter config\n",
    "\n",
    "adapter_config = HoulsbyConfig()\n",
    "model.add_adapter(\"new_syn\", config=adapter_config)\n",
    "model.train_adapter(\"new_syn\")\n",
    "\n",
    "print(model.config)\n",
    "print(model.active_adapters)\n",
    "print(model)\n",
    "print(summary(model, depth=10, row_settings=[\"depth\", \"var_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"/home/chihyuan/Multimodal-Schizo/weights/chinese-electra-108g-small-discriminator_CTB8\",\n",
      "  \"_num_labels\": 2,\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {\n",
      "      \"pfeiffer_inv\": \"9ed5b5a29de19b71\"\n",
      "    },\n",
      "    \"config_map\": {\n",
      "      \"9ed5b5a29de19b71\": {\n",
      "        \"adapter_residual_before_ln\": false,\n",
      "        \"cross_adapter\": false,\n",
      "        \"factorized_phm_W\": true,\n",
      "        \"factorized_phm_rule\": false,\n",
      "        \"hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
      "        \"init_weights\": \"bert\",\n",
      "        \"inv_adapter\": \"nice\",\n",
      "        \"inv_adapter_reduction_factor\": 2,\n",
      "        \"is_parallel\": false,\n",
      "        \"learn_phm\": true,\n",
      "        \"leave_out\": [],\n",
      "        \"ln_after\": false,\n",
      "        \"ln_before\": false,\n",
      "        \"mh_adapter\": false,\n",
      "        \"non_linearity\": \"relu\",\n",
      "        \"original_ln_after\": true,\n",
      "        \"original_ln_before\": true,\n",
      "        \"output_adapter\": true,\n",
      "        \"phm_bias\": true,\n",
      "        \"phm_c_init\": \"normal\",\n",
      "        \"phm_dim\": 4,\n",
      "        \"phm_init_range\": 0.0001,\n",
      "        \"phm_layer\": false,\n",
      "        \"phm_rank\": 1,\n",
      "        \"reduction_factor\": 16,\n",
      "        \"residual_before_ln\": true,\n",
      "        \"scaling\": 1.0,\n",
      "        \"shared_W_phm\": false,\n",
      "        \"shared_phm_rule\": true\n",
      "      }\n",
      "    },\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"prediction_heads\": {\n",
      "    \"pfeiffer_inv\": {\n",
      "      \"activation_function\": \"tanh\",\n",
      "      \"bias\": true,\n",
      "      \"head_type\": \"classification\",\n",
      "      \"label2id\": {\n",
      "        \"LABEL_0\": 0,\n",
      "        \"LABEL_1\": 1\n",
      "      },\n",
      "      \"layers\": 2,\n",
      "      \"num_labels\": 2,\n",
      "      \"use_pooler\": false\n",
      "    }\n",
      "  },\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Stack[pfeiffer_inv]\n",
      "ElectraAdapterModel(\n",
      "  (shared_parameters): ModuleDict()\n",
      "  (electra): ElectraModel(\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (invertible_adapters): ModuleDict(\n",
      "      (pfeiffer_inv): NICECouplingBlock(\n",
      "        (F): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "          (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "        (G): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "          (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (prefix_tuning): PrefixTuningPool(\n",
      "      (prefix_tunings): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (pfeiffer_inv): ClassificationHead(\n",
      "      (0): Dropout(p=0.1, inplace=False)\n",
      "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (2): Activation_Function_Class(\n",
      "        (f): Tanh()\n",
      "      )\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "===================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                                                          Param #\n",
      "===================================================================================================================\n",
      "ElectraAdapterModel                                                                        --\n",
      "├─ModuleDict (shared_parameters): 1-1                                                      --\n",
      "├─ElectraModel (electra): 1-2                                                              --\n",
      "│    └─ModuleDict (shared_parameters): 2-1                                                 --\n",
      "│    └─ModuleDict (invertible_adapters): 2-2                                               --\n",
      "│    │    └─NICECouplingBlock (pfeiffer_inv): 3-1                                          --\n",
      "│    │    │    └─Sequential (F): 4-1                                                       --\n",
      "│    │    │    │    └─Linear (0): 5-1                                                      8,256\n",
      "│    │    │    │    └─Activation_Function_Class (1): 5-2                                   --\n",
      "│    │    │    │    │    └─ReLU (f): 6-1                                                   --\n",
      "│    │    │    │    └─Linear (2): 5-3                                                      8,320\n",
      "│    │    │    └─Sequential (G): 4-2                                                       --\n",
      "│    │    │    │    └─Linear (0): 5-4                                                      8,256\n",
      "│    │    │    │    └─Activation_Function_Class (1): 5-5                                   --\n",
      "│    │    │    │    │    └─ReLU (f): 6-2                                                   --\n",
      "│    │    │    │    └─Linear (2): 5-6                                                      8,320\n",
      "│    └─ElectraEmbeddings (embeddings): 2-3                                                 --\n",
      "│    │    └─Embedding (word_embeddings): 3-2                                               2,704,384\n",
      "│    │    └─Embedding (position_embeddings): 3-3                                           65,536\n",
      "│    │    └─Embedding (token_type_embeddings): 3-4                                         256\n",
      "│    │    └─LayerNorm (LayerNorm): 3-5                                                     256\n",
      "│    │    └─Dropout (dropout): 3-6                                                         --\n",
      "│    └─Linear (embeddings_project): 2-4                                                    33,024\n",
      "│    └─ElectraEncoder (encoder): 2-5                                                       --\n",
      "│    │    └─ModuleList (layer): 3-7                                                        --\n",
      "│    │    │    └─ElectraLayer (0): 4-3                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-7                                    --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-3                                --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-1                                        65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-2                                          65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-3                                        65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-4                                     --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-5                      --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-1                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-1                 --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-4                                 --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-6                                        65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-7                                 512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-8                                     --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-9                                 --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-10                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-8                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-5                                             263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-6                       --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-9                                          --\n",
      "│    │    │    │    │    └─Linear (dense): 6-7                                             262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-8                                      512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-9                                          --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-10                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-11                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-2        --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-2                                    --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-3                        --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-3                                  4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-4               --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-1                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-4                              4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-11                         --\n",
      "│    │    │    └─ElectraLayer (1): 4-4                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-10                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-12                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-12                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-13                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-14                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-15                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-16                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-5                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-5                 --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-13                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-17                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-18                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-19                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-20                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-21                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-11                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-14                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-15                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-12                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-16                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-17                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-18                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-19                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-22                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-6        --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-6                                    --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-7                        --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-7                                  4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-8               --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-2                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-8                              4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-20                         --\n",
      "│    │    │    └─ElectraLayer (2): 4-5                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-13                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-21                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-23                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-24                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-25                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-26                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-27                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-9                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-9                 --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-22                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-28                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-29                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-30                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-31                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-32                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-14                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-23                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-24                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-15                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-25                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-26                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-27                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-28                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-33                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-10       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-10                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-11                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-11                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-12              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-3                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-12                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-29                         --\n",
      "│    │    │    └─ElectraLayer (3): 4-6                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-16                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-30                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-34                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-35                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-36                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-37                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-38                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-13                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-13                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-31                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-39                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-40                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-41                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-42                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-43                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-17                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-32                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-33                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-18                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-34                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-35                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-36                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-37                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-44                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-14       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-14                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-15                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-15                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-16              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-4                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-16                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-38                         --\n",
      "│    │    │    └─ElectraLayer (4): 4-7                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-19                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-39                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-45                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-46                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-47                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-48                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-49                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-17                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-17                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-40                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-50                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-51                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-52                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-53                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-54                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-20                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-41                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-42                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-21                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-43                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-44                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-45                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-46                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-55                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-18       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-18                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-19                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-19                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-20              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-5                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-20                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-47                         --\n",
      "│    │    │    └─ElectraLayer (5): 4-8                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-22                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-48                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-56                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-57                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-58                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-59                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-60                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-21                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-21                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-49                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-61                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-62                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-63                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-64                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-65                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-23                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-50                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-51                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-24                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-52                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-53                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-54                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-55                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-66                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-22       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-22                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-23                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-23                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-24              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-6                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-24                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-56                         --\n",
      "│    │    │    └─ElectraLayer (6): 4-9                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-25                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-57                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-67                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-68                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-69                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-70                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-71                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-25                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-25                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-58                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-72                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-73                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-74                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-75                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-76                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-26                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-59                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-60                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-27                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-61                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-62                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-63                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-64                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-77                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-26       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-26                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-27                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-27                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-28              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-7                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-28                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-65                         --\n",
      "│    │    │    └─ElectraLayer (7): 4-10                                                    --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-28                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-66                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-78                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-79                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-80                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-81                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-82                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-29                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-29                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-67                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-83                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-84                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-85                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-86                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-87                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-29                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-68                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-69                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-30                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-70                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-71                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-72                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-73                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-88                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-30       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-30                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-31                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-31                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-32              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-8                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-32                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-74                         --\n",
      "│    │    │    └─ElectraLayer (8): 4-11                                                    --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-31                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-75                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-89                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-90                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-91                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-92                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-93                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-33                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-33                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-76                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-94                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-95                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-96                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-97                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-98                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-32                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-77                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-78                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-33                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-79                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-80                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-81                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-82                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-99                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-34       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-34                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-35                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-35                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-36              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-9                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-36                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-83                         --\n",
      "│    │    │    └─ElectraLayer (9): 4-12                                                    --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-34                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-84                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-100                                      65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-101                                        65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-102                                      65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-103                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-104                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-37                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-37                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-85                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-105                                      65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-106                               512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-107                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-108                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-109                   --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-35                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-86                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-87                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-36                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-88                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-89                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-90                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-91                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-110                              --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-38       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-38                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-39                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-39                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-40              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-10                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-40                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-92                         --\n",
      "│    │    │    └─ElectraLayer (10): 4-13                                                   --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-37                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-93                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-111                                      65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-112                                        65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-113                                      65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-114                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-115                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-41                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-41                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-94                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-116                                      65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-117                               512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-118                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-119                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-120                   --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-38                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-95                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-96                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-39                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-97                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-98                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-99                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-100                                    --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-121                              --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-42       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-42                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-43                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-43                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-44              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-11                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-44                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-101                        --\n",
      "│    │    │    └─ElectraLayer (11): 4-14                                                   --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-40                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-102                              --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-122                                      65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-123                                        65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-124                                      65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-125                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-126                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-45                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-45                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-103                               --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-127                                      65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-128                               512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-129                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-130                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-131                   --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-41                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-104                                           263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-105                     --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-42                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-106                                           262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-107                                    512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-108                                        --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-109                                    --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-132                              --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-46       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-46                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-47                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-47                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-48              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-12                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-48                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-110                        --\n",
      "│    └─PrefixTuningPool (prefix_tuning): 2-6                                               --\n",
      "│    │    └─ModuleDict (prefix_tunings): 3-8                                               --\n",
      "├─ModuleDict (heads): 1-3                                                                  --\n",
      "│    └─ClassificationHead (pfeiffer_inv): 2-7                                              --\n",
      "│    │    └─Dropout (0): 3-9                                                               --\n",
      "│    │    └─Linear (1): 3-10                                                               65,792\n",
      "│    │    └─Activation_Function_Class (2): 3-11                                            --\n",
      "│    │    │    └─Tanh (f): 4-15                                                            --\n",
      "│    │    └─Dropout (3): 3-12                                                              --\n",
      "│    │    └─Linear (4): 3-13                                                               514\n",
      "===================================================================================================================\n",
      "Total params: 12,481,602\n",
      "Trainable params: 12,481,602\n",
      "Non-trainable params: 0\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    ADAPTER_CONFIG_MAP,\n",
    "    ADAPTER_MODEL_MAPPING,\n",
    "    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n",
    "    AutoAdapterModel,\n",
    "    AutoTokenizer,\n",
    "    HoulsbyConfig,\n",
    "    HoulsbyInvConfig,\n",
    "    MAMConfig,\n",
    "    PfeifferConfig,\n",
    "    PfeifferInvConfig,\n",
    ")\n",
    "\n",
    "model.add_adapter(\"pfeiffer_inv\", config=PfeifferInvConfig())\n",
    "model.add_classification_head(\"pfeiffer_inv\")  # Why must add a head with same name?\n",
    "model.set_active_adapters([\"pfeiffer_inv\"])\n",
    "\n",
    "print(model.config)\n",
    "print(model.active_adapters)\n",
    "print(model)\n",
    "print(summary(model, depth=10, row_settings=[\"depth\", \"var_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits', 'hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input_ids = torch.randint(high=512, size=(2, 20))\n",
    "labels = torch.randint(high=1, size=(2, 1))\n",
    "output = model(input_ids, labels=labels)\n",
    "print(output.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add causal LM head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectraConfig {\n",
      "  \"_name_or_path\": \"/home/chihyuan/Multimodal-Schizo/weights/chinese-electra-108g-small-discriminator_CTB8\",\n",
      "  \"_num_labels\": 2,\n",
      "  \"adapters\": {\n",
      "    \"adapters\": {\n",
      "      \"pfeiffer_inv\": \"9ed5b5a29de19b71\"\n",
      "    },\n",
      "    \"config_map\": {\n",
      "      \"9ed5b5a29de19b71\": {\n",
      "        \"adapter_residual_before_ln\": false,\n",
      "        \"cross_adapter\": false,\n",
      "        \"factorized_phm_W\": true,\n",
      "        \"factorized_phm_rule\": false,\n",
      "        \"hypercomplex_nonlinearity\": \"glorot-uniform\",\n",
      "        \"init_weights\": \"bert\",\n",
      "        \"inv_adapter\": \"nice\",\n",
      "        \"inv_adapter_reduction_factor\": 2,\n",
      "        \"is_parallel\": false,\n",
      "        \"learn_phm\": true,\n",
      "        \"leave_out\": [],\n",
      "        \"ln_after\": false,\n",
      "        \"ln_before\": false,\n",
      "        \"mh_adapter\": false,\n",
      "        \"non_linearity\": \"relu\",\n",
      "        \"original_ln_after\": true,\n",
      "        \"original_ln_before\": true,\n",
      "        \"output_adapter\": true,\n",
      "        \"phm_bias\": true,\n",
      "        \"phm_c_init\": \"normal\",\n",
      "        \"phm_dim\": 4,\n",
      "        \"phm_init_range\": 0.0001,\n",
      "        \"phm_layer\": false,\n",
      "        \"phm_rank\": 1,\n",
      "        \"reduction_factor\": 16,\n",
      "        \"residual_before_ln\": true,\n",
      "        \"scaling\": 1.0,\n",
      "        \"shared_W_phm\": false,\n",
      "        \"shared_phm_rule\": true\n",
      "      }\n",
      "    },\n",
      "    \"fusion_config_map\": {},\n",
      "    \"fusions\": {}\n",
      "  },\n",
      "  \"architectures\": [\n",
      "    \"ElectraModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"label2id\": null,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"prediction_heads\": {\n",
      "    \"pfeiffer_inv\": {\n",
      "      \"activation_function\": \"gelu\",\n",
      "      \"bias\": true,\n",
      "      \"head_type\": \"causal_lm\",\n",
      "      \"label2id\": null,\n",
      "      \"layer_norm\": true,\n",
      "      \"layers\": 2,\n",
      "      \"shift_labels\": true,\n",
      "      \"vocab_size\": 21128\n",
      "    }\n",
      "  },\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "Stack[pfeiffer_inv]\n",
      "ElectraAdapterModel(\n",
      "  (shared_parameters): ModuleDict()\n",
      "  (electra): ElectraModel(\n",
      "    (shared_parameters): ModuleDict()\n",
      "    (invertible_adapters): ModuleDict(\n",
      "      (pfeiffer_inv): NICECouplingBlock(\n",
      "        (F): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "        )\n",
      "        (G): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "          (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (prefix_tuning): PrefixTuningShim(\n",
      "                (pool): PrefixTuningPool(\n",
      "                  (prefix_tunings): ModuleDict()\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (adapters): ModuleDict()\n",
      "              (adapter_fusion_layer): ModuleDict()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (adapters): ModuleDict(\n",
      "              (pfeiffer_inv): Adapter(\n",
      "                (non_linearity): Activation_Function_Class(\n",
      "                  (f): ReLU()\n",
      "                )\n",
      "                (adapter_down): Sequential(\n",
      "                  (0): Linear(in_features=256, out_features=16, bias=True)\n",
      "                  (1): Activation_Function_Class(\n",
      "                    (f): ReLU()\n",
      "                  )\n",
      "                )\n",
      "                (adapter_up): Linear(in_features=16, out_features=256, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (adapter_fusion_layer): ModuleDict()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (prefix_tuning): PrefixTuningPool(\n",
      "      (prefix_tunings): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (pfeiffer_inv): CausalLMHead(\n",
      "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (1): Activation_Function_Class(\n",
      "        (f): GELUActivation()\n",
      "      )\n",
      "      (2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (3): Linear(in_features=128, out_features=21128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "===================================================================================================================\n",
      "Layer (type (var_name):depth-idx)                                                          Param #\n",
      "===================================================================================================================\n",
      "ElectraAdapterModel                                                                        --\n",
      "├─ModuleDict (shared_parameters): 1-1                                                      --\n",
      "├─ElectraModel (electra): 1-2                                                              --\n",
      "│    └─ModuleDict (shared_parameters): 2-1                                                 --\n",
      "│    └─ModuleDict (invertible_adapters): 2-2                                               --\n",
      "│    │    └─NICECouplingBlock (pfeiffer_inv): 3-1                                          --\n",
      "│    │    │    └─Sequential (F): 4-1                                                       --\n",
      "│    │    │    │    └─Linear (0): 5-1                                                      2,080\n",
      "│    │    │    │    └─Activation_Function_Class (1): 5-2                                   --\n",
      "│    │    │    │    │    └─ReLU (f): 6-1                                                   --\n",
      "│    │    │    │    └─Linear (2): 5-3                                                      2,112\n",
      "│    │    │    └─Sequential (G): 4-2                                                       --\n",
      "│    │    │    │    └─Linear (0): 5-4                                                      2,080\n",
      "│    │    │    │    └─Activation_Function_Class (1): 5-5                                   --\n",
      "│    │    │    │    │    └─ReLU (f): 6-2                                                   --\n",
      "│    │    │    │    └─Linear (2): 5-6                                                      2,112\n",
      "│    └─ElectraEmbeddings (embeddings): 2-3                                                 --\n",
      "│    │    └─Embedding (word_embeddings): 3-2                                               2,704,384\n",
      "│    │    └─Embedding (position_embeddings): 3-3                                           65,536\n",
      "│    │    └─Embedding (token_type_embeddings): 3-4                                         256\n",
      "│    │    └─LayerNorm (LayerNorm): 3-5                                                     256\n",
      "│    │    └─Dropout (dropout): 3-6                                                         --\n",
      "│    └─Linear (embeddings_project): 2-4                                                    33,024\n",
      "│    └─ElectraEncoder (encoder): 2-5                                                       --\n",
      "│    │    └─ModuleList (layer): 3-7                                                        --\n",
      "│    │    │    └─ElectraLayer (0): 4-3                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-7                                    --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-3                                --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-1                                        65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-2                                          65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-3                                        65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-4                                     --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-5                      --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-1                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-1                 --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-4                                 --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-6                                        65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-7                                 512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-8                                     --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-9                                 --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-10                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-8                              --\n",
      "│    │    │    │    │    └─Linear (dense): 6-5                                             263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-6                       --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-9                                          --\n",
      "│    │    │    │    │    └─Linear (dense): 6-7                                             262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-8                                      512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-9                                          --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-10                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-11                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-2        --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-2                                    --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-3                        --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-3                                  4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-4               --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-1                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-4                              4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-11                         --\n",
      "│    │    │    └─ElectraLayer (1): 4-4                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-10                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-12                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-12                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-13                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-14                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-15                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-16                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-5                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-5                 --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-13                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-17                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-18                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-19                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-20                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-21                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-11                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-14                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-15                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-12                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-16                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-17                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-18                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-19                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-22                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-6        --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-6                                    --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-7                        --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-7                                  4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-8               --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-2                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-8                              4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-20                         --\n",
      "│    │    │    └─ElectraLayer (2): 4-5                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-13                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-21                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-23                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-24                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-25                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-26                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-27                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-9                          --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-9                 --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-22                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-28                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-29                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-30                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-31                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-32                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-14                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-23                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-24                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-15                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-25                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-26                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-27                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-28                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-33                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-10       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-10                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-11                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-11                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-12              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-3                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-12                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-29                         --\n",
      "│    │    │    └─ElectraLayer (3): 4-6                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-16                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-30                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-34                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-35                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-36                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-37                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-38                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-13                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-13                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-31                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-39                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-40                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-41                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-42                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-43                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-17                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-32                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-33                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-18                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-34                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-35                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-36                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-37                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-44                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-14       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-14                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-15                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-15                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-16              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-4                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-16                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-38                         --\n",
      "│    │    │    └─ElectraLayer (4): 4-7                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-19                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-39                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-45                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-46                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-47                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-48                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-49                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-17                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-17                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-40                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-50                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-51                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-52                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-53                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-54                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-20                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-41                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-42                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-21                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-43                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-44                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-45                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-46                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-55                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-18       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-18                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-19                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-19                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-20              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-5                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-20                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-47                         --\n",
      "│    │    │    └─ElectraLayer (5): 4-8                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-22                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-48                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-56                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-57                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-58                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-59                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-60                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-21                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-21                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-49                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-61                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-62                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-63                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-64                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-65                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-23                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-50                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-51                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-24                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-52                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-53                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-54                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-55                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-66                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-22       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-22                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-23                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-23                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-24              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-6                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-24                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-56                         --\n",
      "│    │    │    └─ElectraLayer (6): 4-9                                                     --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-25                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-57                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-67                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-68                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-69                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-70                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-71                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-25                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-25                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-58                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-72                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-73                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-74                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-75                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-76                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-26                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-59                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-60                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-27                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-61                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-62                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-63                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-64                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-77                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-26       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-26                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-27                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-27                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-28              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-7                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-28                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-65                         --\n",
      "│    │    │    └─ElectraLayer (7): 4-10                                                    --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-28                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-66                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-78                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-79                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-80                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-81                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-82                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-29                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-29                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-67                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-83                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-84                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-85                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-86                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-87                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-29                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-68                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-69                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-30                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-70                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-71                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-72                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-73                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-88                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-30       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-30                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-31                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-31                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-32              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-8                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-32                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-74                         --\n",
      "│    │    │    └─ElectraLayer (8): 4-11                                                    --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-31                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-75                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-89                                       65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-90                                         65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-91                                       65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-92                                    --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-93                     --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-33                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-33                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-76                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-94                                       65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-95                                512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-96                                    --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-97                                --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-98                    --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-32                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-77                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-78                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-33                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-79                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-80                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-81                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-82                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-99                               --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-34       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-34                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-35                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-35                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-36              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-9                              --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-36                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-83                         --\n",
      "│    │    │    └─ElectraLayer (9): 4-12                                                    --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-34                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-84                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-100                                      65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-101                                        65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-102                                      65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-103                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-104                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-37                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-37                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-85                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-105                                      65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-106                               512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-107                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-108                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-109                   --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-35                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-86                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-87                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-36                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-88                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-89                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-90                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-91                                     --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-110                              --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-38       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-38                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-39                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-39                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-40              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-10                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-40                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-92                         --\n",
      "│    │    │    └─ElectraLayer (10): 4-13                                                   --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-37                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-93                               --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-111                                      65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-112                                        65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-113                                      65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-114                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-115                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-41                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-41                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-94                                --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-116                                      65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-117                               512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-118                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-119                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-120                   --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-38                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-95                                            263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-96                      --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-39                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-97                                            262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-98                                     512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-99                                         --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-100                                    --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-121                              --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-42       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-42                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-43                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-43                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-44              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-11                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-44                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-101                        --\n",
      "│    │    │    └─ElectraLayer (11): 4-14                                                   --\n",
      "│    │    │    │    └─ElectraAttention (attention): 5-40                                   --\n",
      "│    │    │    │    │    └─ElectraSelfAttention (self): 6-102                              --\n",
      "│    │    │    │    │    │    └─Linear (query): 7-122                                      65,792\n",
      "│    │    │    │    │    │    └─Linear (key): 7-123                                        65,792\n",
      "│    │    │    │    │    │    └─Linear (value): 7-124                                      65,792\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-125                                   --\n",
      "│    │    │    │    │    │    └─PrefixTuningShim (prefix_tuning): 7-126                    --\n",
      "│    │    │    │    │    │    │    └─PrefixTuningPool (pool): 8-45                         --\n",
      "│    │    │    │    │    │    │    │    └─ModuleDict (prefix_tunings): 9-45                --\n",
      "│    │    │    │    │    └─ElectraSelfOutput (output): 6-103                               --\n",
      "│    │    │    │    │    │    └─Linear (dense): 7-127                                      65,792\n",
      "│    │    │    │    │    │    └─LayerNorm (LayerNorm): 7-128                               512\n",
      "│    │    │    │    │    │    └─Dropout (dropout): 7-129                                   --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapters): 7-130                               --\n",
      "│    │    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 7-131                   --\n",
      "│    │    │    │    └─ElectraIntermediate (intermediate): 5-41                             --\n",
      "│    │    │    │    │    └─Linear (dense): 6-104                                           263,168\n",
      "│    │    │    │    │    └─GELUActivation (intermediate_act_fn): 6-105                     --\n",
      "│    │    │    │    └─ElectraOutput (output): 5-42                                         --\n",
      "│    │    │    │    │    └─Linear (dense): 6-106                                           262,400\n",
      "│    │    │    │    │    └─LayerNorm (LayerNorm): 6-107                                    512\n",
      "│    │    │    │    │    └─Dropout (dropout): 6-108                                        --\n",
      "│    │    │    │    │    └─ModuleDict (adapters): 6-109                                    --\n",
      "│    │    │    │    │    │    └─Adapter (pfeiffer_inv): 7-132                              --\n",
      "│    │    │    │    │    │    │    └─Activation_Function_Class (non_linearity): 8-46       --\n",
      "│    │    │    │    │    │    │    │    └─ReLU (f): 9-46                                   --\n",
      "│    │    │    │    │    │    │    └─Sequential (adapter_down): 8-47                       --\n",
      "│    │    │    │    │    │    │    │    └─Linear (0): 9-47                                 4,112\n",
      "│    │    │    │    │    │    │    │    └─Activation_Function_Class (1): 9-48              --\n",
      "│    │    │    │    │    │    │    │    │    └─ReLU (f): 10-12                             --\n",
      "│    │    │    │    │    │    │    └─Linear (adapter_up): 8-48                             4,352\n",
      "│    │    │    │    │    └─ModuleDict (adapter_fusion_layer): 6-110                        --\n",
      "│    └─PrefixTuningPool (prefix_tuning): 2-6                                               --\n",
      "│    │    └─ModuleDict (prefix_tunings): 3-8                                               --\n",
      "├─ModuleDict (heads): 1-3                                                                  --\n",
      "│    └─CausalLMHead (pfeiffer_inv): 2-7                                                    --\n",
      "│    │    └─Linear (0): 3-9                                                                32,896\n",
      "│    │    └─Activation_Function_Class (1): 3-10                                            --\n",
      "│    │    │    └─GELUActivation (f): 4-15                                                  --\n",
      "│    │    └─LayerNorm (2): 3-11                                                            256\n",
      "│    │    └─Linear (3): 3-12                                                               2,725,512\n",
      "===================================================================================================================\n",
      "Total params: 15,149,192\n",
      "Trainable params: 15,149,192\n",
      "Non-trainable params: 0\n",
      "===================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = AutoAdapterModel.from_pretrained(\n",
    "    \"/home/chihyuan/Multimodal-Schizo/weights/chinese-electra-108g-small-discriminator_CTB8\",\n",
    "    # \"bert-base-uncased\",\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "\n",
    "model.add_adapter(\"pfeiffer_inv\", config=PfeifferInvConfig())\n",
    "model.add_causal_lm_head(\"pfeiffer_inv\")  # Why must add a head with same name?\n",
    "model.set_active_adapters([\"pfeiffer_inv\"])\n",
    "\n",
    "print(model.config)\n",
    "print(model.active_adapters)\n",
    "print(model)\n",
    "print(summary(model, depth=10, row_settings=[\"depth\", \"var_names\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['loss', 'logits', 'hidden_states'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "input_ids = torch.randint(high=512, size=(2, 20))\n",
    "labels = torch.randint(high=1, size=(2, 20))\n",
    "output = model(input_ids, labels=labels)\n",
    "print(output.keys())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59c55fd4b3978c0def058b870785763f0b3768b417004cdd166463757ca86d4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Multimodal_Schizo_adapter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
