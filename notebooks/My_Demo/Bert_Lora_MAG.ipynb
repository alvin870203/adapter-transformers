{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/chihyuan/Multimodal-Schizo/weights/chinese-bert-wwm-scz were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /home/chihyuan/Multimodal-Schizo/weights/chinese-bert-wwm-scz and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model_name = \"/home/chihyuan/Multimodal-Schizo/weights/chinese-bert-wwm-scz\"\n",
    "# model_name = \"/home/chihyuan/Multimodal-Schizo/weights/chinese-electra-108g-small-discriminator_CTB8\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "adapter_name = \"sem\"\n",
    "model.add_adapter(adapter_name, config=\"lora\")\n",
    "model.train_adapter(adapter_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "BertModel                                                    --\n",
       "├─ModuleDict: 1-1                                            --\n",
       "├─ModuleDict: 1-2                                            --\n",
       "├─BertEmbeddings: 1-3                                        --\n",
       "│    └─Embedding: 2-1                                        (16,226,304)\n",
       "│    └─Embedding: 2-2                                        (393,216)\n",
       "│    └─Embedding: 2-3                                        (1,536)\n",
       "│    └─LayerNorm: 2-4                                        (1,536)\n",
       "│    └─Dropout: 2-5                                          --\n",
       "├─BertEncoder: 1-4                                           --\n",
       "│    └─ModuleList: 2-6                                       --\n",
       "│    │    └─BertLayer: 3-1                                   --\n",
       "│    │    │    └─BertAttention: 4-1                          --\n",
       "│    │    │    │    └─BertSelfAttention: 5-1                 --\n",
       "│    │    │    │    │    └─Linear: 6-1                       590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-1              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-1               12,288\n",
       "│    │    │    │    │    └─Linear: 6-2                       590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-2              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-2               12,288\n",
       "│    │    │    │    │    └─Linear: 6-3                       590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-3              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-3               12,288\n",
       "│    │    │    │    │    └─Dropout: 6-4                      --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-5             --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-4        --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-4         --\n",
       "│    │    │    │    └─BertSelfOutput: 5-2                    --\n",
       "│    │    │    │    │    └─Linear: 6-6                       (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-7                    (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-8                      --\n",
       "│    │    │    │    │    └─ModuleDict: 6-9                   --\n",
       "│    │    │    │    │    └─ModuleDict: 6-10                  --\n",
       "│    │    │    └─BertIntermediate: 4-2                       --\n",
       "│    │    │    │    └─Linear: 5-3                            (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-11                  --\n",
       "│    │    │    │    └─GELUActivation: 5-4                    --\n",
       "│    │    │    └─BertOutput: 4-3                             --\n",
       "│    │    │    │    └─Linear: 5-5                            (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-12                  --\n",
       "│    │    │    │    └─LayerNorm: 5-6                         (1,536)\n",
       "│    │    │    │    └─Dropout: 5-7                           --\n",
       "│    │    │    │    └─ModuleDict: 5-8                        --\n",
       "│    │    │    │    └─ModuleDict: 5-9                        --\n",
       "│    │    └─BertLayer: 3-2                                   --\n",
       "│    │    │    └─BertAttention: 4-4                          --\n",
       "│    │    │    │    └─BertSelfAttention: 5-10                --\n",
       "│    │    │    │    │    └─Linear: 6-13                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-5              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-5               12,288\n",
       "│    │    │    │    │    └─Linear: 6-14                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-6              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-6               12,288\n",
       "│    │    │    │    │    └─Linear: 6-15                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-7              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-7               12,288\n",
       "│    │    │    │    │    └─Dropout: 6-16                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-17            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-8        --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-8         --\n",
       "│    │    │    │    └─BertSelfOutput: 5-11                   --\n",
       "│    │    │    │    │    └─Linear: 6-18                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-19                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-20                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-21                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-22                  --\n",
       "│    │    │    └─BertIntermediate: 4-5                       --\n",
       "│    │    │    │    └─Linear: 5-12                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-23                  --\n",
       "│    │    │    │    └─GELUActivation: 5-13                   --\n",
       "│    │    │    └─BertOutput: 4-6                             --\n",
       "│    │    │    │    └─Linear: 5-14                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-24                  --\n",
       "│    │    │    │    └─LayerNorm: 5-15                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-16                          --\n",
       "│    │    │    │    └─ModuleDict: 5-17                       --\n",
       "│    │    │    │    └─ModuleDict: 5-18                       --\n",
       "│    │    └─BertLayer: 3-3                                   --\n",
       "│    │    │    └─BertAttention: 4-7                          --\n",
       "│    │    │    │    └─BertSelfAttention: 5-19                --\n",
       "│    │    │    │    │    └─Linear: 6-25                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-9              --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-9               12,288\n",
       "│    │    │    │    │    └─Linear: 6-26                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-10             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-10              12,288\n",
       "│    │    │    │    │    └─Linear: 6-27                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-11             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-11              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-28                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-29            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-12       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-12        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-20                   --\n",
       "│    │    │    │    │    └─Linear: 6-30                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-31                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-32                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-33                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-34                  --\n",
       "│    │    │    └─BertIntermediate: 4-8                       --\n",
       "│    │    │    │    └─Linear: 5-21                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-35                  --\n",
       "│    │    │    │    └─GELUActivation: 5-22                   --\n",
       "│    │    │    └─BertOutput: 4-9                             --\n",
       "│    │    │    │    └─Linear: 5-23                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-36                  --\n",
       "│    │    │    │    └─LayerNorm: 5-24                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-25                          --\n",
       "│    │    │    │    └─ModuleDict: 5-26                       --\n",
       "│    │    │    │    └─ModuleDict: 5-27                       --\n",
       "│    │    └─BertLayer: 3-4                                   --\n",
       "│    │    │    └─BertAttention: 4-10                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-28                --\n",
       "│    │    │    │    │    └─Linear: 6-37                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-13             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-13              12,288\n",
       "│    │    │    │    │    └─Linear: 6-38                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-14             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-14              12,288\n",
       "│    │    │    │    │    └─Linear: 6-39                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-15             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-15              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-40                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-41            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-16       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-16        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-29                   --\n",
       "│    │    │    │    │    └─Linear: 6-42                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-43                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-44                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-45                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-46                  --\n",
       "│    │    │    └─BertIntermediate: 4-11                      --\n",
       "│    │    │    │    └─Linear: 5-30                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-47                  --\n",
       "│    │    │    │    └─GELUActivation: 5-31                   --\n",
       "│    │    │    └─BertOutput: 4-12                            --\n",
       "│    │    │    │    └─Linear: 5-32                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-48                  --\n",
       "│    │    │    │    └─LayerNorm: 5-33                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-34                          --\n",
       "│    │    │    │    └─ModuleDict: 5-35                       --\n",
       "│    │    │    │    └─ModuleDict: 5-36                       --\n",
       "│    │    └─BertLayer: 3-5                                   --\n",
       "│    │    │    └─BertAttention: 4-13                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-37                --\n",
       "│    │    │    │    │    └─Linear: 6-49                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-17             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-17              12,288\n",
       "│    │    │    │    │    └─Linear: 6-50                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-18             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-18              12,288\n",
       "│    │    │    │    │    └─Linear: 6-51                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-19             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-19              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-52                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-53            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-20       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-20        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-38                   --\n",
       "│    │    │    │    │    └─Linear: 6-54                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-55                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-56                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-57                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-58                  --\n",
       "│    │    │    └─BertIntermediate: 4-14                      --\n",
       "│    │    │    │    └─Linear: 5-39                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-59                  --\n",
       "│    │    │    │    └─GELUActivation: 5-40                   --\n",
       "│    │    │    └─BertOutput: 4-15                            --\n",
       "│    │    │    │    └─Linear: 5-41                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-60                  --\n",
       "│    │    │    │    └─LayerNorm: 5-42                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-43                          --\n",
       "│    │    │    │    └─ModuleDict: 5-44                       --\n",
       "│    │    │    │    └─ModuleDict: 5-45                       --\n",
       "│    │    └─BertLayer: 3-6                                   --\n",
       "│    │    │    └─BertAttention: 4-16                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-46                --\n",
       "│    │    │    │    │    └─Linear: 6-61                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-21             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-21              12,288\n",
       "│    │    │    │    │    └─Linear: 6-62                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-22             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-22              12,288\n",
       "│    │    │    │    │    └─Linear: 6-63                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-23             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-23              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-64                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-65            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-24       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-24        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-47                   --\n",
       "│    │    │    │    │    └─Linear: 6-66                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-67                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-68                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-69                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-70                  --\n",
       "│    │    │    └─BertIntermediate: 4-17                      --\n",
       "│    │    │    │    └─Linear: 5-48                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-71                  --\n",
       "│    │    │    │    └─GELUActivation: 5-49                   --\n",
       "│    │    │    └─BertOutput: 4-18                            --\n",
       "│    │    │    │    └─Linear: 5-50                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-72                  --\n",
       "│    │    │    │    └─LayerNorm: 5-51                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-52                          --\n",
       "│    │    │    │    └─ModuleDict: 5-53                       --\n",
       "│    │    │    │    └─ModuleDict: 5-54                       --\n",
       "│    │    └─BertLayer: 3-7                                   --\n",
       "│    │    │    └─BertAttention: 4-19                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-55                --\n",
       "│    │    │    │    │    └─Linear: 6-73                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-25             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-25              12,288\n",
       "│    │    │    │    │    └─Linear: 6-74                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-26             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-26              12,288\n",
       "│    │    │    │    │    └─Linear: 6-75                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-27             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-27              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-76                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-77            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-28       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-28        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-56                   --\n",
       "│    │    │    │    │    └─Linear: 6-78                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-79                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-80                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-81                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-82                  --\n",
       "│    │    │    └─BertIntermediate: 4-20                      --\n",
       "│    │    │    │    └─Linear: 5-57                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-83                  --\n",
       "│    │    │    │    └─GELUActivation: 5-58                   --\n",
       "│    │    │    └─BertOutput: 4-21                            --\n",
       "│    │    │    │    └─Linear: 5-59                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-84                  --\n",
       "│    │    │    │    └─LayerNorm: 5-60                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-61                          --\n",
       "│    │    │    │    └─ModuleDict: 5-62                       --\n",
       "│    │    │    │    └─ModuleDict: 5-63                       --\n",
       "│    │    └─BertLayer: 3-8                                   --\n",
       "│    │    │    └─BertAttention: 4-22                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-64                --\n",
       "│    │    │    │    │    └─Linear: 6-85                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-29             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-29              12,288\n",
       "│    │    │    │    │    └─Linear: 6-86                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-30             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-30              12,288\n",
       "│    │    │    │    │    └─Linear: 6-87                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-31             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-31              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-88                     --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-89            --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-32       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-32        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-65                   --\n",
       "│    │    │    │    │    └─Linear: 6-90                      (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-91                   (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-92                     --\n",
       "│    │    │    │    │    └─ModuleDict: 6-93                  --\n",
       "│    │    │    │    │    └─ModuleDict: 6-94                  --\n",
       "│    │    │    └─BertIntermediate: 4-23                      --\n",
       "│    │    │    │    └─Linear: 5-66                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-95                  --\n",
       "│    │    │    │    └─GELUActivation: 5-67                   --\n",
       "│    │    │    └─BertOutput: 4-24                            --\n",
       "│    │    │    │    └─Linear: 5-68                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-96                  --\n",
       "│    │    │    │    └─LayerNorm: 5-69                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-70                          --\n",
       "│    │    │    │    └─ModuleDict: 5-71                       --\n",
       "│    │    │    │    └─ModuleDict: 5-72                       --\n",
       "│    │    └─BertLayer: 3-9                                   --\n",
       "│    │    │    └─BertAttention: 4-25                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-73                --\n",
       "│    │    │    │    │    └─Linear: 6-97                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-33             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-33              12,288\n",
       "│    │    │    │    │    └─Linear: 6-98                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-34             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-34              12,288\n",
       "│    │    │    │    │    └─Linear: 6-99                      590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-35             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-35              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-100                    --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-101           --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-36       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-36        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-74                   --\n",
       "│    │    │    │    │    └─Linear: 6-102                     (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-103                  (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-104                    --\n",
       "│    │    │    │    │    └─ModuleDict: 6-105                 --\n",
       "│    │    │    │    │    └─ModuleDict: 6-106                 --\n",
       "│    │    │    └─BertIntermediate: 4-26                      --\n",
       "│    │    │    │    └─Linear: 5-75                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-107                 --\n",
       "│    │    │    │    └─GELUActivation: 5-76                   --\n",
       "│    │    │    └─BertOutput: 4-27                            --\n",
       "│    │    │    │    └─Linear: 5-77                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-108                 --\n",
       "│    │    │    │    └─LayerNorm: 5-78                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-79                          --\n",
       "│    │    │    │    └─ModuleDict: 5-80                       --\n",
       "│    │    │    │    └─ModuleDict: 5-81                       --\n",
       "│    │    └─BertLayer: 3-10                                  --\n",
       "│    │    │    └─BertAttention: 4-28                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-82                --\n",
       "│    │    │    │    │    └─Linear: 6-109                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-37             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-37              12,288\n",
       "│    │    │    │    │    └─Linear: 6-110                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-38             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-38              12,288\n",
       "│    │    │    │    │    └─Linear: 6-111                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-39             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-39              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-112                    --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-113           --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-40       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-40        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-83                   --\n",
       "│    │    │    │    │    └─Linear: 6-114                     (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-115                  (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-116                    --\n",
       "│    │    │    │    │    └─ModuleDict: 6-117                 --\n",
       "│    │    │    │    │    └─ModuleDict: 6-118                 --\n",
       "│    │    │    └─BertIntermediate: 4-29                      --\n",
       "│    │    │    │    └─Linear: 5-84                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-119                 --\n",
       "│    │    │    │    └─GELUActivation: 5-85                   --\n",
       "│    │    │    └─BertOutput: 4-30                            --\n",
       "│    │    │    │    └─Linear: 5-86                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-120                 --\n",
       "│    │    │    │    └─LayerNorm: 5-87                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-88                          --\n",
       "│    │    │    │    └─ModuleDict: 5-89                       --\n",
       "│    │    │    │    └─ModuleDict: 5-90                       --\n",
       "│    │    └─BertLayer: 3-11                                  --\n",
       "│    │    │    └─BertAttention: 4-31                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-91                --\n",
       "│    │    │    │    │    └─Linear: 6-121                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-41             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-41              12,288\n",
       "│    │    │    │    │    └─Linear: 6-122                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-42             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-42              12,288\n",
       "│    │    │    │    │    └─Linear: 6-123                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-43             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-43              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-124                    --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-125           --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-44       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-44        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-92                   --\n",
       "│    │    │    │    │    └─Linear: 6-126                     (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-127                  (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-128                    --\n",
       "│    │    │    │    │    └─ModuleDict: 6-129                 --\n",
       "│    │    │    │    │    └─ModuleDict: 6-130                 --\n",
       "│    │    │    └─BertIntermediate: 4-32                      --\n",
       "│    │    │    │    └─Linear: 5-93                           (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-131                 --\n",
       "│    │    │    │    └─GELUActivation: 5-94                   --\n",
       "│    │    │    └─BertOutput: 4-33                            --\n",
       "│    │    │    │    └─Linear: 5-95                           (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-132                 --\n",
       "│    │    │    │    └─LayerNorm: 5-96                        (1,536)\n",
       "│    │    │    │    └─Dropout: 5-97                          --\n",
       "│    │    │    │    └─ModuleDict: 5-98                       --\n",
       "│    │    │    │    └─ModuleDict: 5-99                       --\n",
       "│    │    └─BertLayer: 3-12                                  --\n",
       "│    │    │    └─BertAttention: 4-34                         --\n",
       "│    │    │    │    └─BertSelfAttention: 5-100               --\n",
       "│    │    │    │    │    └─Linear: 6-133                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-45             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-45              12,288\n",
       "│    │    │    │    │    └─Linear: 6-134                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-46             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-46              12,288\n",
       "│    │    │    │    │    └─Linear: 6-135                     590,592\n",
       "│    │    │    │    │    │    └─ModuleDict: 7-47             --\n",
       "│    │    │    │    │    │    │    └─LoRA: 8-47              12,288\n",
       "│    │    │    │    │    └─Dropout: 6-136                    --\n",
       "│    │    │    │    │    └─PrefixTuningShim: 6-137           --\n",
       "│    │    │    │    │    │    └─PrefixTuningPool: 7-48       --\n",
       "│    │    │    │    │    │    │    └─ModuleDict: 8-48        --\n",
       "│    │    │    │    └─BertSelfOutput: 5-101                  --\n",
       "│    │    │    │    │    └─Linear: 6-138                     (590,592)\n",
       "│    │    │    │    │    └─LayerNorm: 6-139                  (1,536)\n",
       "│    │    │    │    │    └─Dropout: 6-140                    --\n",
       "│    │    │    │    │    └─ModuleDict: 6-141                 --\n",
       "│    │    │    │    │    └─ModuleDict: 6-142                 --\n",
       "│    │    │    └─BertIntermediate: 4-35                      --\n",
       "│    │    │    │    └─Linear: 5-102                          (2,362,368)\n",
       "│    │    │    │    │    └─ModuleDict: 6-143                 --\n",
       "│    │    │    │    └─GELUActivation: 5-103                  --\n",
       "│    │    │    └─BertOutput: 4-36                            --\n",
       "│    │    │    │    └─Linear: 5-104                          (2,360,064)\n",
       "│    │    │    │    │    └─ModuleDict: 6-144                 --\n",
       "│    │    │    │    └─LayerNorm: 5-105                       (1,536)\n",
       "│    │    │    │    └─Dropout: 5-106                         --\n",
       "│    │    │    │    └─ModuleDict: 5-107                      --\n",
       "│    │    │    │    └─ModuleDict: 5-108                      --\n",
       "├─BertPooler: 1-5                                            --\n",
       "│    └─Linear: 2-7                                           (590,592)\n",
       "│    └─Tanh: 2-8                                             --\n",
       "├─PrefixTuningPool: 1-6                                      --\n",
       "│    └─ModuleDict: 2-9                                       --\n",
       "=====================================================================================\n",
       "Total params: 102,710,016\n",
       "Trainable params: 442,368\n",
       "Non-trainable params: 102,267,648\n",
       "====================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(model.dummy_inputs[\"input_ids\"].shape)\n",
    "model(**model.dummy_inputs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "bottleneck = model.config.adapters.get('sem').r\n",
    "print(bottleneck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.6276, -0.2011,  1.3188,  ...,  0.0538,  0.5368,  0.1084],\n",
       "         [ 0.1923, -0.0663,  0.6771,  ..., -0.0265,  0.8241, -0.1615],\n",
       "         [ 0.2777, -0.0309,  0.7634,  ..., -0.2681,  0.4358, -0.2162],\n",
       "         [ 0.5428, -0.6433,  0.3406,  ...,  0.1040,  0.3127, -0.3458],\n",
       "         [ 0.3783, -0.2226, -0.3764,  ..., -0.5578,  0.2540,  0.1097]],\n",
       "\n",
       "        [[-0.3147,  0.2326,  0.0459,  ...,  0.8017,  0.3678,  0.7561],\n",
       "         [ 0.5693, -0.2490,  0.5930,  ...,  0.0416,  0.1855,  0.6724],\n",
       "         [ 0.7443, -0.1090,  0.3682,  ...,  0.1279,  0.7396,  0.5392],\n",
       "         [ 0.8801, -0.0728,  0.3263,  ...,  0.4415,  0.4591,  0.5949],\n",
       "         [ 0.4163,  0.0422,  0.5707,  ..., -0.0746,  0.3046,  0.8143]],\n",
       "\n",
       "        [[ 1.2520,  0.2067, -0.1161,  ...,  0.4083,  1.3596,  0.0548],\n",
       "         [ 0.6400, -0.3296,  0.3792,  ..., -0.0022,  0.5273, -0.3587],\n",
       "         [ 1.3201, -0.0526, -0.1606,  ...,  0.0429,  0.6319, -0.2509],\n",
       "         [ 0.9929, -0.3953,  0.0230,  ..., -0.1861, -0.0419, -0.3419],\n",
       "         [ 1.0474, -0.2111, -0.2128,  ..., -0.3226,  0.9061, -0.1543]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.2616,  0.1558, -0.1841,  ...,  0.4621, -0.0538, -0.1519],\n",
       "        [-0.1943,  0.0667, -0.2985,  ...,  0.9129, -0.5600, -0.0584],\n",
       "        [-0.2771,  0.0408, -0.0701,  ...,  0.3676, -0.4315,  0.1173]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=(tensor([[[-0.0709,  0.4743, -0.6127,  ...,  0.9273, -0.0567, -0.0525],\n",
       "         [-0.4144,  0.3176, -0.0180,  ...,  0.7743, -0.4012,  0.6330],\n",
       "         [ 0.6165,  0.4129, -0.4190,  ...,  1.3247,  0.1765,  0.6666],\n",
       "         [ 0.7094,  0.1524,  0.1136,  ...,  1.6618, -0.1672,  1.0848],\n",
       "         [-0.3168,  0.5922, -0.0758,  ...,  0.7579,  0.6435,  0.5169]],\n",
       "\n",
       "        [[-0.0000,  0.4406, -0.0434,  ...,  0.0000, -0.2927,  0.1309],\n",
       "         [ 0.0972, -0.0223,  0.0130,  ...,  0.9420,  0.0252,  0.5273],\n",
       "         [ 0.2945,  0.3630, -0.3140,  ...,  1.4110,  0.0000,  0.4522],\n",
       "         [ 0.7094,  0.1524,  0.1136,  ...,  1.6618, -0.1672,  1.0848],\n",
       "         [ 0.5170,  0.5643, -0.3108,  ...,  0.0000,  0.5088,  0.2972]],\n",
       "\n",
       "        [[ 0.3221,  0.4178, -0.0000,  ...,  1.2379, -0.3957, -0.0330],\n",
       "         [ 0.4009,  0.3921, -0.2133,  ...,  1.0170, -0.4696,  0.3946],\n",
       "         [ 0.6165,  0.4129, -0.4190,  ...,  1.3247,  0.0000,  0.6666],\n",
       "         [ 0.3282,  0.0781, -0.4556,  ...,  1.4160, -0.2918,  1.2290],\n",
       "         [-0.1957,  0.3713, -0.3055,  ...,  0.9995,  0.9066,  0.2769]]]), tensor([[[-0.0572,  0.6391, -0.7917,  ...,  0.2828, -0.3556, -0.0781],\n",
       "         [-0.3659,  0.3757, -0.5156,  ..., -0.4818, -0.5124,  0.4897],\n",
       "         [ 0.3595,  1.4178, -0.7158,  ...,  0.7471,  0.0752,  0.4125],\n",
       "         [ 0.5551,  0.2201, -0.0759,  ...,  1.2490, -0.5775,  1.2562],\n",
       "         [-0.1728,  0.3514, -0.4871,  ..., -0.3490,  0.7223,  0.3498]],\n",
       "\n",
       "        [[ 0.5974,  0.6046, -0.6204,  ...,  0.3407, -0.0145, -0.5037],\n",
       "         [ 0.2901,  0.9808, -0.0342,  ...,  0.4262, -0.1218,  0.2631],\n",
       "         [ 0.1686,  0.4100, -0.8832,  ...,  0.5051,  0.0226,  0.2140],\n",
       "         [ 0.3258,  0.5237, -0.3886,  ...,  0.7760, -0.4164,  1.2081],\n",
       "         [ 0.3869,  1.9677, -0.9580,  ..., -0.2423,  0.0481,  0.0848]],\n",
       "\n",
       "        [[ 0.3894,  0.4352, -0.1369,  ...,  0.3841, -0.3537, -0.2054],\n",
       "         [ 0.3844,  1.1978, -0.6842,  ...,  0.4729, -0.9063,  0.3419],\n",
       "         [ 0.5833,  1.0538, -0.8347,  ...,  0.6074, -0.3939,  1.2007],\n",
       "         [ 0.4409,  0.4582, -0.6769,  ...,  0.9132, -0.3419,  1.5351],\n",
       "         [ 0.2474,  0.7010, -0.8841,  ..., -0.1846,  0.5133,  0.6227]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1876,  0.5632, -0.5599,  ...,  0.0489,  0.2825,  0.2909],\n",
       "         [ 0.1258,  0.2983, -0.2618,  ..., -0.3187,  0.2792,  0.1892],\n",
       "         [ 0.6435,  1.5119, -0.5840,  ...,  0.5133,  0.7221,  0.3238],\n",
       "         [ 0.9525,  0.4716,  0.1975,  ...,  0.9065,  0.1599,  1.4045],\n",
       "         [-0.1599,  0.4399, -0.3070,  ...,  0.0115,  1.0773,  0.4157]],\n",
       "\n",
       "        [[ 0.6190,  0.9813, -0.0520,  ..., -0.1260,  0.5534, -0.2051],\n",
       "         [ 0.1262,  1.2415, -0.1630,  ...,  0.0499,  0.4699,  0.1981],\n",
       "         [-0.2424,  0.6497, -1.1938,  ..., -0.0792,  0.7418,  0.1511],\n",
       "         [ 0.4598,  0.7796, -0.2053,  ...,  0.2146,  0.1853,  1.0526],\n",
       "         [ 0.2219,  1.9251, -0.4799,  ..., -0.2042,  0.8792,  0.2001]],\n",
       "\n",
       "        [[ 0.4213,  0.2135, -0.0791,  ...,  0.0733, -0.0258,  0.0598],\n",
       "         [ 0.6256,  0.9308, -0.3941,  ...,  0.0831, -0.6311,  0.1874],\n",
       "         [ 0.4886,  1.1539, -0.6004,  ...,  0.6822,  0.1287,  1.3146],\n",
       "         [ 0.8890,  0.6732, -0.6155,  ...,  0.6804,  0.0082,  1.7932],\n",
       "         [ 0.6655,  0.9016, -0.4251,  ..., -0.4806,  0.4316,  0.7703]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1898,  0.7144, -0.6077,  ...,  0.2774,  0.0482,  0.2801],\n",
       "         [ 0.5300,  0.4530,  0.0134,  ...,  0.0073, -0.0177, -0.1793],\n",
       "         [ 0.8121,  1.4826, -0.2622,  ...,  0.0365,  0.3109,  0.1072],\n",
       "         [ 1.0171,  0.6441,  0.5290,  ...,  0.2939, -0.2363,  1.0916],\n",
       "         [-0.0512,  0.7946, -0.2303,  ..., -0.4508,  0.4730,  0.1973]],\n",
       "\n",
       "        [[ 0.6479,  0.7752, -0.1421,  ..., -0.5591, -0.1482, -0.0106],\n",
       "         [ 0.0579,  1.1691, -0.2049,  ..., -0.3434, -0.2817, -0.0096],\n",
       "         [-0.2467,  0.7774, -1.2222,  ..., -0.1753, -0.0587, -0.1979],\n",
       "         [ 0.4149,  0.7654, -0.4422,  ..., -0.3846,  0.1673,  0.5326],\n",
       "         [ 0.4836,  1.7560, -0.7271,  ..., -0.2930,  0.2155, -0.0653]],\n",
       "\n",
       "        [[ 0.6121,  0.1241, -0.0313,  ...,  0.0105, -0.1915,  0.0577],\n",
       "         [ 0.6976,  0.6099, -0.0155,  ...,  0.0085, -0.5530,  0.2235],\n",
       "         [ 1.0504,  1.1164, -0.3269,  ...,  0.2352,  0.1179,  1.1578],\n",
       "         [ 1.0317,  0.5199, -0.3207,  ..., -0.0095, -0.3073,  1.4466],\n",
       "         [ 0.9985,  0.6289, -0.4738,  ..., -0.4006,  0.0705,  0.6303]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.6140,  0.3820, -0.7218,  ...,  0.5856, -0.3364, -0.4436],\n",
       "         [ 0.7820,  0.4660, -0.1326,  ...,  0.2212, -0.5596, -0.2747],\n",
       "         [ 1.2455,  1.4560, -0.4159,  ...,  0.5117,  0.2967, -0.8428],\n",
       "         [ 1.2034,  0.6803, -0.0237,  ...,  0.6047, -0.1926, -0.1280],\n",
       "         [ 0.6461,  0.2711, -0.5033,  ...,  0.0927,  0.1094, -0.1694]],\n",
       "\n",
       "        [[ 0.4410,  0.4792, -0.7658,  ...,  0.5849, -0.7083, -0.1435],\n",
       "         [ 0.0329,  0.8716, -0.1269,  ...,  0.6523, -0.5902, -0.6003],\n",
       "         [-0.3621,  1.0058, -1.2000,  ...,  0.6502, -0.9155, -0.4149],\n",
       "         [ 0.2946,  0.9268, -0.6942,  ...,  0.8929, -0.6354,  0.0787],\n",
       "         [ 0.5380,  1.6941, -1.0116,  ...,  0.9170, -0.5993,  0.0089]],\n",
       "\n",
       "        [[ 0.4898,  0.2176, -0.3521,  ..., -0.0074, -0.4681, -0.3817],\n",
       "         [ 0.6323,  0.5005, -0.2374,  ...,  0.1257, -0.8196, -0.1544],\n",
       "         [ 1.0634,  0.9241, -0.7122,  ...,  0.5347,  0.0099,  0.4475],\n",
       "         [ 0.9814,  0.4752, -0.4289,  ...,  0.2004, -0.5380,  0.7994],\n",
       "         [ 1.2382,  0.5005, -0.6894,  ...,  0.2508, -0.3061, -0.0759]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.6089,  0.1933, -0.5828,  ..., -0.1589,  0.2720, -0.7525],\n",
       "         [ 1.7515,  0.4672, -0.0801,  ..., -0.4482,  0.5814,  0.1868],\n",
       "         [ 2.3710,  0.9451, -0.4793,  ..., -0.4236,  1.1017, -0.6590],\n",
       "         [ 2.2625,  0.6253,  0.0606,  ..., -0.1110,  0.3690, -0.3515],\n",
       "         [ 1.8533, -0.0573, -0.3667,  ..., -0.2805,  0.0763, -0.2606]],\n",
       "\n",
       "        [[ 1.2558, -0.7747, -0.6717,  ...,  0.6091, -0.1585, -0.1650],\n",
       "         [-0.1658,  0.1535, -0.4939,  ...,  0.5199, -0.0959, -0.5222],\n",
       "         [-0.3701,  0.5030, -1.0843,  ...,  0.1190, -0.3026, -0.1577],\n",
       "         [ 1.0109,  0.1456, -0.3006,  ...,  0.5968, -0.1178,  0.3188],\n",
       "         [ 1.1210,  0.8247, -0.7737,  ...,  0.5562,  0.0485, -0.1980]],\n",
       "\n",
       "        [[ 0.9382,  0.2974, -0.1831,  ...,  0.1367, -0.7799, -0.2422],\n",
       "         [ 1.0661,  0.5515, -0.1847,  ...,  0.4674, -0.4744,  0.1539],\n",
       "         [ 1.4974,  0.9279, -0.9087,  ...,  0.4706,  0.4381,  0.6142],\n",
       "         [ 1.4704,  0.7215, -0.4116,  ...,  0.4521, -0.3879,  0.8280],\n",
       "         [ 1.3668,  0.2989, -0.7104,  ...,  0.2171,  0.1096,  0.5075]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.7861,  0.6143, -0.4781,  ...,  0.0918, -0.1570, -1.2663],\n",
       "         [ 1.7309,  0.2666,  0.1098,  ...,  0.7017,  0.0496,  0.2163],\n",
       "         [ 2.3026,  1.2260, -0.3736,  ..., -0.0698,  0.4452, -1.0085],\n",
       "         [ 2.2437,  0.4821,  0.1131,  ...,  0.4292, -0.0473, -0.1947],\n",
       "         [ 1.2827, -0.2089, -0.3379,  ...,  0.3125, -0.2932, -0.8265]],\n",
       "\n",
       "        [[ 1.1112, -0.2275, -1.1255,  ...,  0.1855, -0.2644, -0.6573],\n",
       "         [ 0.0682, -0.1195, -0.5558,  ...,  0.1514, -0.0134, -0.7284],\n",
       "         [ 0.2824,  0.0068, -1.3401,  ...,  0.1094, -0.1685, -0.1561],\n",
       "         [ 1.0805, -0.1870, -0.4979,  ...,  0.6702,  0.1104,  0.6173],\n",
       "         [ 1.3338,  0.1005, -0.6848,  ...,  0.6203,  0.0165, -0.0868]],\n",
       "\n",
       "        [[ 1.1491,  0.3086,  0.1003,  ..., -0.0152, -0.8524, -0.2323],\n",
       "         [ 0.9214,  0.3766,  0.1864,  ...,  0.4963, -0.2599, -0.1835],\n",
       "         [ 1.4399,  0.7013, -0.7821,  ...,  0.3571,  0.5657,  0.3525],\n",
       "         [ 1.5361,  0.6164,  0.0604,  ..., -0.0407, -0.4419,  0.2934],\n",
       "         [ 1.3825, -0.1114, -0.2489,  ..., -0.2259,  0.1318,  0.4418]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.4115e+00,  2.7061e-01, -1.1825e+00,  ..., -1.2529e-01,\n",
       "          -7.0523e-01, -5.5359e-01],\n",
       "         [ 6.3272e-01,  1.7545e-03, -4.3162e-01,  ...,  2.3496e-01,\n",
       "           1.1521e-02,  8.1440e-01],\n",
       "         [ 6.8528e-01,  1.0491e+00, -6.0704e-01,  ..., -2.9432e-01,\n",
       "          -5.6477e-02, -4.5176e-02],\n",
       "         [ 1.1490e+00,  6.1211e-01, -6.2088e-01,  ..., -6.3567e-01,\n",
       "          -6.4368e-01,  7.2100e-02],\n",
       "         [ 8.7329e-01,  6.6974e-02, -4.8987e-01,  ...,  1.6692e-01,\n",
       "          -1.3659e+00, -1.1165e-01]],\n",
       "\n",
       "        [[ 5.0227e-01, -2.2576e-02, -8.5068e-01,  ..., -4.3242e-01,\n",
       "          -3.8928e-01,  5.1540e-02],\n",
       "         [-3.1569e-01,  1.9144e-01, -3.5988e-01,  ..., -8.8857e-01,\n",
       "           4.7206e-03,  1.0995e-01],\n",
       "         [-4.1582e-01, -6.0925e-01, -1.0481e+00,  ..., -2.8905e-02,\n",
       "           3.6179e-01,  2.8298e-01],\n",
       "         [ 8.0466e-01, -3.2407e-02, -2.3224e-01,  ...,  2.1043e-01,\n",
       "          -3.7166e-01,  7.7807e-01],\n",
       "         [ 9.2080e-01,  7.6707e-01, -5.9262e-01,  ...,  3.4105e-01,\n",
       "          -1.2640e-01,  9.7437e-02]],\n",
       "\n",
       "        [[ 7.4512e-01,  2.9593e-01, -5.4494e-01,  ...,  3.0407e-02,\n",
       "          -1.0589e+00,  3.7097e-01],\n",
       "         [ 5.8515e-01,  5.0286e-01,  1.0599e-01,  ...,  5.4331e-01,\n",
       "          -5.6466e-01,  2.5491e-01],\n",
       "         [ 1.8553e+00,  9.9683e-01, -7.4304e-01,  ...,  1.3034e-01,\n",
       "          -4.6186e-01,  6.2292e-01],\n",
       "         [ 1.5444e+00,  4.3322e-01, -1.6429e-01,  ...,  4.9704e-02,\n",
       "          -1.4052e+00,  4.5191e-01],\n",
       "         [ 1.0863e+00,  4.0950e-01, -5.4391e-01,  ...,  4.9515e-01,\n",
       "          -1.1519e+00,  1.0143e+00]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5600, -0.3679, -0.4888,  ..., -0.5839, -0.4064, -0.1573],\n",
       "         [ 0.0311, -0.5565, -0.5145,  ..., -0.2018,  0.2209,  0.5060],\n",
       "         [-0.0237,  0.5631, -0.7413,  ..., -0.6186,  0.1314,  0.2604],\n",
       "         [ 0.1790,  0.0369, -0.4679,  ..., -1.2889,  0.0842,  0.2999],\n",
       "         [ 0.4461, -0.4446, -0.4390,  ..., -1.6389, -0.5160,  0.1986]],\n",
       "\n",
       "        [[ 0.7201, -0.2196, -0.2222,  ..., -0.2723, -0.0881, -0.2281],\n",
       "         [ 0.1206, -0.2214, -0.4117,  ..., -1.4625,  0.6805,  0.1949],\n",
       "         [ 0.4294, -0.8361, -0.9207,  ..., -0.8105,  0.0775,  0.5898],\n",
       "         [ 1.0059, -0.6410, -0.1819,  ..., -1.4564, -0.4273,  0.6626],\n",
       "         [ 0.6488,  0.3275, -0.6835,  ..., -0.7219, -0.3890,  0.3646]],\n",
       "\n",
       "        [[ 1.0256,  0.0702, -0.8278,  ..., -0.9364,  0.0360,  0.3804],\n",
       "         [ 0.7162, -0.3307,  0.0846,  ..., -0.1416, -0.3271,  0.1556],\n",
       "         [ 1.9061,  0.8435, -0.8681,  ..., -0.8399, -0.6784,  0.4997],\n",
       "         [ 1.4483, -0.2710, -0.0783,  ..., -0.6445, -0.7456,  0.1257],\n",
       "         [ 1.1446,  0.1242, -0.6923,  ..., -0.3311, -0.4711,  0.9936]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 9.3983e-02,  3.8027e-01,  6.7955e-02,  ..., -5.7785e-01,\n",
       "          -1.8146e-01, -8.7689e-01],\n",
       "         [-8.5273e-02,  1.7474e-01, -7.7750e-01,  ..., -9.0410e-01,\n",
       "           5.8720e-01, -3.1095e-01],\n",
       "         [-3.9008e-01,  1.3336e+00, -8.9615e-01,  ..., -7.3022e-01,\n",
       "          -4.0078e-02, -2.7863e-01],\n",
       "         [ 3.5910e-01,  4.2066e-01, -7.7266e-01,  ..., -8.8412e-01,\n",
       "          -1.4740e-03, -1.8226e-01],\n",
       "         [ 3.2854e-03,  3.1642e-01, -3.4167e-01,  ..., -1.4053e+00,\n",
       "          -4.1002e-01,  1.2357e-01]],\n",
       "\n",
       "        [[ 5.2722e-01,  3.5327e-01, -3.8754e-01,  ...,  2.9507e-01,\n",
       "          -3.4319e-01,  5.6723e-01],\n",
       "         [ 3.1611e-01, -3.5543e-01, -8.9317e-01,  ..., -6.8645e-01,\n",
       "          -1.6339e-01,  2.2355e-01],\n",
       "         [ 1.1018e+00, -6.2110e-01, -1.1802e+00,  ..., -7.0390e-01,\n",
       "          -5.2143e-01,  4.2574e-01],\n",
       "         [ 1.2833e+00, -4.7570e-01, -4.8953e-01,  ..., -9.1027e-01,\n",
       "          -8.4532e-01,  7.0982e-01],\n",
       "         [ 4.3123e-01,  2.7148e-01, -6.6943e-01,  ..., -3.6489e-01,\n",
       "          -1.4405e+00,  5.2699e-01]],\n",
       "\n",
       "        [[ 1.4311e+00,  2.3716e-01, -5.9575e-01,  ..., -6.4242e-01,\n",
       "           2.9064e-01,  3.6728e-01],\n",
       "         [ 1.6629e+00,  2.0432e-01, -2.9763e-01,  ..., -2.0325e-01,\n",
       "          -2.2596e-01, -1.7085e-01],\n",
       "         [ 2.1807e+00,  8.2018e-01, -9.9713e-01,  ...,  1.0402e-01,\n",
       "          -3.7611e-01, -1.4223e-01],\n",
       "         [ 1.6919e+00,  4.7508e-01, -4.7533e-01,  ..., -3.0372e-01,\n",
       "          -2.5334e-01, -3.8742e-01],\n",
       "         [ 1.3975e+00,  2.7827e-01, -9.6349e-01,  ..., -7.8319e-02,\n",
       "          -2.5191e-02,  6.8255e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.4115,  0.1714,  0.2389,  ..., -0.0030, -0.7124, -0.4831],\n",
       "         [ 0.1127,  0.1489, -0.9927,  ...,  0.0854,  0.2111, -0.1663],\n",
       "         [-0.1278,  0.8295, -0.6647,  ..., -0.3116,  0.0796, -0.7667],\n",
       "         [ 0.3427, -0.0739, -0.9601,  ..., -0.3790, -0.0028, -0.5259],\n",
       "         [-0.1734,  0.5508, -0.3994,  ..., -0.5506, -0.8502, -0.1572]],\n",
       "\n",
       "        [[ 0.3196,  0.0401,  0.0504,  ...,  1.1081, -0.5284,  0.4213],\n",
       "         [ 0.9748, -0.5524, -0.9896,  ..., -0.5913,  0.4543,  0.5294],\n",
       "         [ 1.5755, -0.0738, -0.6924,  ..., -0.4302,  0.3883,  0.6945],\n",
       "         [ 1.4447, -0.1567, -1.0375,  ..., -0.3687, -0.1320,  0.6653],\n",
       "         [ 0.9754,  0.4229, -0.4956,  ...,  0.0067, -0.5067,  0.7414]],\n",
       "\n",
       "        [[ 1.8388,  0.6643, -0.4159,  ...,  0.0658,  0.6184,  0.2060],\n",
       "         [ 1.8527,  0.2272, -0.1871,  ...,  0.3742, -0.5435, -0.7010],\n",
       "         [ 2.2269,  0.4453, -0.6660,  ...,  0.6366, -0.4025, -0.1314],\n",
       "         [ 2.0205,  0.1720, -0.4658,  ...,  0.5680, -0.3363, -0.5755],\n",
       "         [ 1.8843,  0.4479, -0.9851,  ...,  0.6581, -0.0971,  0.1288]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3343, -0.3422,  1.3731,  ..., -0.6949, -0.2419, -0.7293],\n",
       "         [-0.0538,  0.0457,  0.3434,  ..., -0.3460,  0.4280, -0.7265],\n",
       "         [-0.0577,  0.2558,  0.5552,  ..., -0.6212, -0.0417, -1.0838],\n",
       "         [ 0.3051, -1.0422,  0.2039,  ..., -0.3747,  0.0813, -1.1663],\n",
       "         [ 0.0279, -0.2467,  0.0675,  ..., -1.0136, -0.5194, -0.8866]],\n",
       "\n",
       "        [[ 0.0103, -0.1988,  1.1760,  ...,  0.5852, -0.3204,  0.7951],\n",
       "         [ 1.0914, -0.3479,  0.2888,  ..., -0.6256,  0.3220,  0.8644],\n",
       "         [ 1.5075, -0.1276,  0.7368,  ..., -0.6346,  0.2895,  0.8785],\n",
       "         [ 1.5023,  0.0091,  0.0696,  ..., -0.2595, -0.3125,  0.7523],\n",
       "         [ 1.2354,  0.3820,  0.9644,  ..., -0.4599, -0.3160,  1.0336]],\n",
       "\n",
       "        [[ 1.9351,  0.5864,  0.8091,  ..., -0.5410,  0.6330,  0.3821],\n",
       "         [ 1.7009, -0.1714,  0.8724,  ..., -0.4478, -0.4691, -0.6019],\n",
       "         [ 2.5314, -0.0356, -0.0166,  ..., -0.0991, -0.1504, -0.4561],\n",
       "         [ 1.8311, -0.0858,  0.5198,  ..., -0.4496, -0.2559, -0.6731],\n",
       "         [ 1.9763, -0.1990, -0.2201,  ..., -0.1504,  0.0880,  0.2348]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.6276, -0.2011,  1.3188,  ...,  0.0538,  0.5368,  0.1084],\n",
       "         [ 0.1923, -0.0663,  0.6771,  ..., -0.0265,  0.8241, -0.1615],\n",
       "         [ 0.2777, -0.0309,  0.7634,  ..., -0.2681,  0.4358, -0.2162],\n",
       "         [ 0.5428, -0.6433,  0.3406,  ...,  0.1040,  0.3127, -0.3458],\n",
       "         [ 0.3783, -0.2226, -0.3764,  ..., -0.5578,  0.2540,  0.1097]],\n",
       "\n",
       "        [[-0.3147,  0.2326,  0.0459,  ...,  0.8017,  0.3678,  0.7561],\n",
       "         [ 0.5693, -0.2490,  0.5930,  ...,  0.0416,  0.1855,  0.6724],\n",
       "         [ 0.7443, -0.1090,  0.3682,  ...,  0.1279,  0.7396,  0.5392],\n",
       "         [ 0.8801, -0.0728,  0.3263,  ...,  0.4415,  0.4591,  0.5949],\n",
       "         [ 0.4163,  0.0422,  0.5707,  ..., -0.0746,  0.3046,  0.8143]],\n",
       "\n",
       "        [[ 1.2520,  0.2067, -0.1161,  ...,  0.4083,  1.3596,  0.0548],\n",
       "         [ 0.6400, -0.3296,  0.3792,  ..., -0.0022,  0.5273, -0.3587],\n",
       "         [ 1.3201, -0.0526, -0.1606,  ...,  0.0429,  0.6319, -0.2509],\n",
       "         [ 0.9929, -0.3953,  0.0230,  ..., -0.1861, -0.0419, -0.3419],\n",
       "         [ 1.0474, -0.2111, -0.2128,  ..., -0.3226,  0.9061, -0.1543]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "acu_vis_dict = {0: torch.randn((bottleneck))}\n",
    "model(**model.dummy_inputs, acu_vis_dict=acu_vis_dict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59c55fd4b3978c0def058b870785763f0b3768b417004cdd166463757ca86d4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Multimodal_Schizo_adapter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
